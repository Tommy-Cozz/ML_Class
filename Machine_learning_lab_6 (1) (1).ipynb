{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Problem 1 Hidden layers 1\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# XOR training data\n",
        "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_train = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create a sequential model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(2,)),                       #Input Layer of 2 neruons\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid'),    #Hidden Layer of 2 neruons\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')     #Output layer of one neuron\n",
        "])\n",
        "\n",
        "# Compile the model with Stochastic Gradient Descent optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=5), loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with stochastic gradient descent\n",
        "model.fit(X_train, y_train, epochs=150, verbose=2, batch_size=1)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "predictions = model.predict(X_train)\n",
        "print(\"Final Predictions:\")\n",
        "print(np.round(predictions))\n"
      ],
      "metadata": {
        "id": "2gskXzEk1hyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 1 Hidden Layers 2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# XOR training data\n",
        "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_train = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create a sequential model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(2,)),                       #Input Layer of 2 neruons\n",
        "    tf.keras.layers.Dense(units=2, activation='sigmoid'),    #Hidden Layer of 2 neruons\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')     #Output layer of one neuron\n",
        "])\n",
        "\n",
        "# Compile the model with Stochastic Gradient Descent optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=5), loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with stochastic gradient descent\n",
        "model.fit(X_train, y_train, epochs=150, verbose=2, batch_size=1)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "predictions = model.predict(X_train)\n",
        "print(\"Final Predictions:\")\n",
        "print(np.round(predictions))\n"
      ],
      "metadata": {
        "id": "UpKjpwUhnn-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 1 Hidden layers 8\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# XOR training data\n",
        "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_train = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create a sequential model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(2,)),                       #Input Layer of 2 neruons\n",
        "    tf.keras.layers.Dense(units=8, activation='sigmoid'),    #Hidden Layer of 2 neruons\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')     #Output layer of one neuron\n",
        "])\n",
        "\n",
        "# Compile the model with Stochastic Gradient Descent optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=5), loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with stochastic gradient descent\n",
        "model.fit(X_train, y_train, epochs=150, verbose=2, batch_size=1)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "predictions = model.predict(X_train)\n",
        "print(\"Final Predictions:\")\n",
        "print(np.round(predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE2j_ZAR2ow7",
        "outputId": "06012af9-2c30-476f-94ea-4381701426c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "4/4 - 0s - loss: 0.6263 - accuracy: 0.2500 - 477ms/epoch - 119ms/step\n",
            "Epoch 2/150\n",
            "4/4 - 0s - loss: 0.5241 - accuracy: 0.2500 - 14ms/epoch - 4ms/step\n",
            "Epoch 3/150\n",
            "4/4 - 0s - loss: 0.6063 - accuracy: 0.2500 - 16ms/epoch - 4ms/step\n",
            "Epoch 4/150\n",
            "4/4 - 0s - loss: 0.5255 - accuracy: 0.2500 - 16ms/epoch - 4ms/step\n",
            "Epoch 5/150\n",
            "4/4 - 0s - loss: 0.4470 - accuracy: 0.5000 - 16ms/epoch - 4ms/step\n",
            "Epoch 6/150\n",
            "4/4 - 0s - loss: 0.5593 - accuracy: 0.0000e+00 - 15ms/epoch - 4ms/step\n",
            "Epoch 7/150\n",
            "4/4 - 0s - loss: 0.5002 - accuracy: 0.2500 - 16ms/epoch - 4ms/step\n",
            "Epoch 8/150\n",
            "4/4 - 0s - loss: 0.4809 - accuracy: 0.2500 - 19ms/epoch - 5ms/step\n",
            "Epoch 9/150\n",
            "4/4 - 0s - loss: 0.4882 - accuracy: 0.0000e+00 - 13ms/epoch - 3ms/step\n",
            "Epoch 10/150\n",
            "4/4 - 0s - loss: 0.4944 - accuracy: 0.0000e+00 - 16ms/epoch - 4ms/step\n",
            "Epoch 11/150\n",
            "4/4 - 0s - loss: 0.3789 - accuracy: 0.2500 - 18ms/epoch - 4ms/step\n",
            "Epoch 12/150\n",
            "4/4 - 0s - loss: 0.4062 - accuracy: 0.5000 - 14ms/epoch - 4ms/step\n",
            "Epoch 13/150\n",
            "4/4 - 0s - loss: 0.4598 - accuracy: 0.0000e+00 - 15ms/epoch - 4ms/step\n",
            "Epoch 14/150\n",
            "4/4 - 0s - loss: 0.4495 - accuracy: 0.2500 - 14ms/epoch - 3ms/step\n",
            "Epoch 15/150\n",
            "4/4 - 0s - loss: 0.3585 - accuracy: 0.2500 - 13ms/epoch - 3ms/step\n",
            "Epoch 16/150\n",
            "4/4 - 0s - loss: 0.3269 - accuracy: 0.5000 - 14ms/epoch - 4ms/step\n",
            "Epoch 17/150\n",
            "4/4 - 0s - loss: 0.4078 - accuracy: 0.5000 - 13ms/epoch - 3ms/step\n",
            "Epoch 18/150\n",
            "4/4 - 0s - loss: 0.4442 - accuracy: 0.5000 - 15ms/epoch - 4ms/step\n",
            "Epoch 19/150\n",
            "4/4 - 0s - loss: 0.3486 - accuracy: 0.2500 - 13ms/epoch - 3ms/step\n",
            "Epoch 20/150\n",
            "4/4 - 0s - loss: 0.3279 - accuracy: 0.5000 - 17ms/epoch - 4ms/step\n",
            "Epoch 21/150\n",
            "4/4 - 0s - loss: 0.4770 - accuracy: 0.0000e+00 - 11ms/epoch - 3ms/step\n",
            "Epoch 22/150\n",
            "4/4 - 0s - loss: 0.3229 - accuracy: 0.5000 - 11ms/epoch - 3ms/step\n",
            "Epoch 23/150\n",
            "4/4 - 0s - loss: 0.3963 - accuracy: 0.5000 - 13ms/epoch - 3ms/step\n",
            "Epoch 24/150\n",
            "4/4 - 0s - loss: 0.4453 - accuracy: 0.2500 - 13ms/epoch - 3ms/step\n",
            "Epoch 25/150\n",
            "4/4 - 0s - loss: 0.3509 - accuracy: 0.2500 - 15ms/epoch - 4ms/step\n",
            "Epoch 26/150\n",
            "4/4 - 0s - loss: 0.3222 - accuracy: 0.5000 - 15ms/epoch - 4ms/step\n",
            "Epoch 27/150\n",
            "4/4 - 0s - loss: 0.4159 - accuracy: 0.2500 - 12ms/epoch - 3ms/step\n",
            "Epoch 28/150\n",
            "4/4 - 0s - loss: 0.3422 - accuracy: 0.5000 - 12ms/epoch - 3ms/step\n",
            "Epoch 29/150\n",
            "4/4 - 0s - loss: 0.3208 - accuracy: 0.5000 - 10ms/epoch - 3ms/step\n",
            "Epoch 30/150\n",
            "4/4 - 0s - loss: 0.4173 - accuracy: 0.2500 - 10ms/epoch - 3ms/step\n",
            "Epoch 31/150\n",
            "4/4 - 0s - loss: 0.4430 - accuracy: 0.2500 - 10ms/epoch - 3ms/step\n",
            "Epoch 32/150\n",
            "4/4 - 0s - loss: 0.4457 - accuracy: 0.0000e+00 - 10ms/epoch - 3ms/step\n",
            "Epoch 33/150\n",
            "4/4 - 0s - loss: 0.4302 - accuracy: 0.2500 - 11ms/epoch - 3ms/step\n",
            "Epoch 34/150\n",
            "4/4 - 0s - loss: 0.4298 - accuracy: 0.0000e+00 - 11ms/epoch - 3ms/step\n",
            "Epoch 35/150\n",
            "4/4 - 0s - loss: 0.3319 - accuracy: 0.5000 - 10ms/epoch - 3ms/step\n",
            "Epoch 36/150\n",
            "4/4 - 0s - loss: 0.4495 - accuracy: 0.0000e+00 - 10ms/epoch - 2ms/step\n",
            "Epoch 37/150\n",
            "4/4 - 0s - loss: 0.4087 - accuracy: 0.2500 - 11ms/epoch - 3ms/step\n",
            "Epoch 38/150\n",
            "4/4 - 0s - loss: 0.4009 - accuracy: 0.2500 - 10ms/epoch - 3ms/step\n",
            "Epoch 39/150\n",
            "4/4 - 0s - loss: 0.4353 - accuracy: 0.0000e+00 - 10ms/epoch - 3ms/step\n",
            "Epoch 40/150\n",
            "4/4 - 0s - loss: 0.3355 - accuracy: 0.5000 - 11ms/epoch - 3ms/step\n",
            "Epoch 41/150\n",
            "4/4 - 0s - loss: 0.3183 - accuracy: 0.5000 - 10ms/epoch - 3ms/step\n",
            "Epoch 42/150\n",
            "4/4 - 0s - loss: 0.3837 - accuracy: 0.2500 - 10ms/epoch - 3ms/step\n",
            "Epoch 43/150\n",
            "4/4 - 0s - loss: 0.4004 - accuracy: 0.0000e+00 - 11ms/epoch - 3ms/step\n",
            "Epoch 44/150\n",
            "4/4 - 0s - loss: 0.3301 - accuracy: 0.5000 - 11ms/epoch - 3ms/step\n",
            "Epoch 45/150\n",
            "4/4 - 0s - loss: 0.3662 - accuracy: 0.2500 - 10ms/epoch - 3ms/step\n",
            "Epoch 46/150\n",
            "4/4 - 0s - loss: 0.3307 - accuracy: 0.5000 - 10ms/epoch - 3ms/step\n",
            "Epoch 47/150\n",
            "4/4 - 0s - loss: 0.3301 - accuracy: 0.5000 - 11ms/epoch - 3ms/step\n",
            "Epoch 48/150\n",
            "4/4 - 0s - loss: 0.2874 - accuracy: 0.5000 - 10ms/epoch - 3ms/step\n",
            "Epoch 49/150\n",
            "4/4 - 0s - loss: 0.3547 - accuracy: 0.5000 - 10ms/epoch - 3ms/step\n",
            "Epoch 50/150\n",
            "4/4 - 0s - loss: 0.2042 - accuracy: 0.5000 - 10ms/epoch - 3ms/step\n",
            "Epoch 51/150\n",
            "4/4 - 0s - loss: 0.3634 - accuracy: 0.5000 - 10ms/epoch - 3ms/step\n",
            "Epoch 52/150\n",
            "4/4 - 0s - loss: 0.2748 - accuracy: 0.7500 - 11ms/epoch - 3ms/step\n",
            "Epoch 53/150\n",
            "4/4 - 0s - loss: 0.2497 - accuracy: 0.7500 - 10ms/epoch - 3ms/step\n",
            "Epoch 54/150\n",
            "4/4 - 0s - loss: 0.2412 - accuracy: 0.7500 - 30ms/epoch - 7ms/step\n",
            "Epoch 55/150\n",
            "4/4 - 0s - loss: 0.2888 - accuracy: 0.5000 - 29ms/epoch - 7ms/step\n",
            "Epoch 56/150\n",
            "4/4 - 0s - loss: 0.2298 - accuracy: 0.7500 - 34ms/epoch - 8ms/step\n",
            "Epoch 57/150\n",
            "4/4 - 0s - loss: 0.2819 - accuracy: 0.5000 - 34ms/epoch - 8ms/step\n",
            "Epoch 58/150\n",
            "4/4 - 0s - loss: 0.2478 - accuracy: 0.5000 - 34ms/epoch - 8ms/step\n",
            "Epoch 59/150\n",
            "4/4 - 0s - loss: 0.2089 - accuracy: 0.7500 - 27ms/epoch - 7ms/step\n",
            "Epoch 60/150\n",
            "4/4 - 0s - loss: 0.2654 - accuracy: 0.7500 - 40ms/epoch - 10ms/step\n",
            "Epoch 61/150\n",
            "4/4 - 0s - loss: 0.2346 - accuracy: 0.7500 - 59ms/epoch - 15ms/step\n",
            "Epoch 62/150\n",
            "4/4 - 0s - loss: 0.2316 - accuracy: 0.7500 - 32ms/epoch - 8ms/step\n",
            "Epoch 63/150\n",
            "4/4 - 0s - loss: 0.2248 - accuracy: 0.7500 - 17ms/epoch - 4ms/step\n",
            "Epoch 64/150\n",
            "4/4 - 0s - loss: 0.2156 - accuracy: 0.7500 - 25ms/epoch - 6ms/step\n",
            "Epoch 65/150\n",
            "4/4 - 0s - loss: 0.2562 - accuracy: 0.5000 - 26ms/epoch - 6ms/step\n",
            "Epoch 66/150\n",
            "4/4 - 0s - loss: 0.2171 - accuracy: 0.7500 - 25ms/epoch - 6ms/step\n",
            "Epoch 67/150\n",
            "4/4 - 0s - loss: 0.2195 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 68/150\n",
            "4/4 - 0s - loss: 0.2494 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 69/150\n",
            "4/4 - 0s - loss: 0.2411 - accuracy: 0.5000 - 24ms/epoch - 6ms/step\n",
            "Epoch 70/150\n",
            "4/4 - 0s - loss: 0.2276 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 71/150\n",
            "4/4 - 0s - loss: 0.2244 - accuracy: 0.7500 - 40ms/epoch - 10ms/step\n",
            "Epoch 72/150\n",
            "4/4 - 0s - loss: 0.2337 - accuracy: 0.7500 - 30ms/epoch - 7ms/step\n",
            "Epoch 73/150\n",
            "4/4 - 0s - loss: 0.2123 - accuracy: 0.7500 - 14ms/epoch - 4ms/step\n",
            "Epoch 74/150\n",
            "4/4 - 0s - loss: 0.2153 - accuracy: 0.7500 - 20ms/epoch - 5ms/step\n",
            "Epoch 75/150\n",
            "4/4 - 0s - loss: 0.2278 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 76/150\n",
            "4/4 - 0s - loss: 0.2481 - accuracy: 0.7500 - 32ms/epoch - 8ms/step\n",
            "Epoch 77/150\n",
            "4/4 - 0s - loss: 0.2359 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 78/150\n",
            "4/4 - 0s - loss: 0.2263 - accuracy: 0.7500 - 90ms/epoch - 23ms/step\n",
            "Epoch 79/150\n",
            "4/4 - 0s - loss: 0.2228 - accuracy: 0.7500 - 32ms/epoch - 8ms/step\n",
            "Epoch 80/150\n",
            "4/4 - 0s - loss: 0.2071 - accuracy: 0.7500 - 31ms/epoch - 8ms/step\n",
            "Epoch 81/150\n",
            "4/4 - 0s - loss: 0.2599 - accuracy: 0.5000 - 30ms/epoch - 7ms/step\n",
            "Epoch 82/150\n",
            "4/4 - 0s - loss: 0.2094 - accuracy: 0.7500 - 23ms/epoch - 6ms/step\n",
            "Epoch 83/150\n",
            "4/4 - 0s - loss: 0.2160 - accuracy: 0.7500 - 22ms/epoch - 6ms/step\n",
            "Epoch 84/150\n",
            "4/4 - 0s - loss: 0.2196 - accuracy: 0.7500 - 34ms/epoch - 9ms/step\n",
            "Epoch 85/150\n",
            "4/4 - 0s - loss: 0.2211 - accuracy: 0.7500 - 20ms/epoch - 5ms/step\n",
            "Epoch 86/150\n",
            "4/4 - 0s - loss: 0.2475 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 87/150\n",
            "4/4 - 0s - loss: 0.2088 - accuracy: 0.7500 - 28ms/epoch - 7ms/step\n",
            "Epoch 88/150\n",
            "4/4 - 0s - loss: 0.2564 - accuracy: 0.5000 - 15ms/epoch - 4ms/step\n",
            "Epoch 89/150\n",
            "4/4 - 0s - loss: 0.2227 - accuracy: 0.7500 - 49ms/epoch - 12ms/step\n",
            "Epoch 90/150\n",
            "4/4 - 0s - loss: 0.2321 - accuracy: 0.7500 - 42ms/epoch - 10ms/step\n",
            "Epoch 91/150\n",
            "4/4 - 0s - loss: 0.2203 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 92/150\n",
            "4/4 - 0s - loss: 0.2191 - accuracy: 0.7500 - 17ms/epoch - 4ms/step\n",
            "Epoch 93/150\n",
            "4/4 - 0s - loss: 0.2241 - accuracy: 0.7500 - 29ms/epoch - 7ms/step\n",
            "Epoch 94/150\n",
            "4/4 - 0s - loss: 0.2068 - accuracy: 0.7500 - 35ms/epoch - 9ms/step\n",
            "Epoch 95/150\n",
            "4/4 - 0s - loss: 0.2361 - accuracy: 0.7500 - 32ms/epoch - 8ms/step\n",
            "Epoch 96/150\n",
            "4/4 - 0s - loss: 0.2256 - accuracy: 0.7500 - 23ms/epoch - 6ms/step\n",
            "Epoch 97/150\n",
            "4/4 - 0s - loss: 0.2050 - accuracy: 0.7500 - 36ms/epoch - 9ms/step\n",
            "Epoch 98/150\n",
            "4/4 - 0s - loss: 0.2359 - accuracy: 0.7500 - 35ms/epoch - 9ms/step\n",
            "Epoch 99/150\n",
            "4/4 - 0s - loss: 0.2316 - accuracy: 0.5000 - 21ms/epoch - 5ms/step\n",
            "Epoch 100/150\n",
            "4/4 - 0s - loss: 0.2227 - accuracy: 0.7500 - 23ms/epoch - 6ms/step\n",
            "Epoch 101/150\n",
            "4/4 - 0s - loss: 0.2217 - accuracy: 0.7500 - 22ms/epoch - 5ms/step\n",
            "Epoch 102/150\n",
            "4/4 - 0s - loss: 0.2160 - accuracy: 0.7500 - 19ms/epoch - 5ms/step\n",
            "Epoch 103/150\n",
            "4/4 - 0s - loss: 0.2024 - accuracy: 0.7500 - 21ms/epoch - 5ms/step\n",
            "Epoch 104/150\n",
            "4/4 - 0s - loss: 0.2444 - accuracy: 0.5000 - 27ms/epoch - 7ms/step\n",
            "Epoch 105/150\n",
            "4/4 - 0s - loss: 0.2014 - accuracy: 0.7500 - 19ms/epoch - 5ms/step\n",
            "Epoch 106/150\n",
            "4/4 - 0s - loss: 0.2422 - accuracy: 0.5000 - 51ms/epoch - 13ms/step\n",
            "Epoch 107/150\n",
            "4/4 - 0s - loss: 0.2127 - accuracy: 0.7500 - 64ms/epoch - 16ms/step\n",
            "Epoch 108/150\n",
            "4/4 - 0s - loss: 0.2246 - accuracy: 0.7500 - 24ms/epoch - 6ms/step\n",
            "Epoch 109/150\n",
            "4/4 - 0s - loss: 0.2206 - accuracy: 0.7500 - 30ms/epoch - 8ms/step\n",
            "Epoch 110/150\n",
            "4/4 - 0s - loss: 0.1984 - accuracy: 0.7500 - 20ms/epoch - 5ms/step\n",
            "Epoch 111/150\n",
            "4/4 - 0s - loss: 0.2377 - accuracy: 0.5000 - 25ms/epoch - 6ms/step\n",
            "Epoch 112/150\n",
            "4/4 - 0s - loss: 0.2196 - accuracy: 0.7500 - 23ms/epoch - 6ms/step\n",
            "Epoch 113/150\n",
            "4/4 - 0s - loss: 0.2221 - accuracy: 0.7500 - 24ms/epoch - 6ms/step\n",
            "Epoch 114/150\n",
            "4/4 - 0s - loss: 0.2164 - accuracy: 0.7500 - 25ms/epoch - 6ms/step\n",
            "Epoch 115/150\n",
            "4/4 - 0s - loss: 0.2090 - accuracy: 0.7500 - 22ms/epoch - 6ms/step\n",
            "Epoch 116/150\n",
            "4/4 - 0s - loss: 0.1843 - accuracy: 0.7500 - 25ms/epoch - 6ms/step\n",
            "Epoch 117/150\n",
            "4/4 - 0s - loss: 0.2121 - accuracy: 0.7500 - 24ms/epoch - 6ms/step\n",
            "Epoch 118/150\n",
            "4/4 - 0s - loss: 0.2332 - accuracy: 0.5000 - 25ms/epoch - 6ms/step\n",
            "Epoch 119/150\n",
            "4/4 - 0s - loss: 0.2066 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 120/150\n",
            "4/4 - 0s - loss: 0.1784 - accuracy: 0.7500 - 15ms/epoch - 4ms/step\n",
            "Epoch 121/150\n",
            "4/4 - 0s - loss: 0.1958 - accuracy: 0.7500 - 17ms/epoch - 4ms/step\n",
            "Epoch 122/150\n",
            "4/4 - 0s - loss: 0.1926 - accuracy: 0.7500 - 15ms/epoch - 4ms/step\n",
            "Epoch 123/150\n",
            "4/4 - 0s - loss: 0.2054 - accuracy: 0.7500 - 22ms/epoch - 6ms/step\n",
            "Epoch 124/150\n",
            "4/4 - 0s - loss: 0.1680 - accuracy: 0.7500 - 17ms/epoch - 4ms/step\n",
            "Epoch 125/150\n",
            "4/4 - 0s - loss: 0.1454 - accuracy: 0.7500 - 15ms/epoch - 4ms/step\n",
            "Epoch 126/150\n",
            "4/4 - 0s - loss: 0.1584 - accuracy: 0.7500 - 15ms/epoch - 4ms/step\n",
            "Epoch 127/150\n",
            "4/4 - 0s - loss: 0.2032 - accuracy: 0.7500 - 17ms/epoch - 4ms/step\n",
            "Epoch 128/150\n",
            "4/4 - 0s - loss: 0.1186 - accuracy: 0.7500 - 16ms/epoch - 4ms/step\n",
            "Epoch 129/150\n",
            "4/4 - 0s - loss: 0.1265 - accuracy: 0.7500 - 20ms/epoch - 5ms/step\n",
            "Epoch 130/150\n",
            "4/4 - 0s - loss: 0.1145 - accuracy: 0.7500 - 15ms/epoch - 4ms/step\n",
            "Epoch 131/150\n",
            "4/4 - 0s - loss: 0.1755 - accuracy: 0.7500 - 21ms/epoch - 5ms/step\n",
            "Epoch 132/150\n",
            "4/4 - 0s - loss: 0.0910 - accuracy: 1.0000 - 18ms/epoch - 4ms/step\n",
            "Epoch 133/150\n",
            "4/4 - 0s - loss: 0.0652 - accuracy: 1.0000 - 23ms/epoch - 6ms/step\n",
            "Epoch 134/150\n",
            "4/4 - 0s - loss: 0.0432 - accuracy: 1.0000 - 30ms/epoch - 7ms/step\n",
            "Epoch 135/150\n",
            "4/4 - 0s - loss: 0.0311 - accuracy: 1.0000 - 15ms/epoch - 4ms/step\n",
            "Epoch 136/150\n",
            "4/4 - 0s - loss: 0.0256 - accuracy: 1.0000 - 15ms/epoch - 4ms/step\n",
            "Epoch 137/150\n",
            "4/4 - 0s - loss: 0.0221 - accuracy: 1.0000 - 20ms/epoch - 5ms/step\n",
            "Epoch 138/150\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 1.0000 - 26ms/epoch - 6ms/step\n",
            "Epoch 139/150\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 1.0000 - 19ms/epoch - 5ms/step\n",
            "Epoch 140/150\n",
            "4/4 - 0s - loss: 0.0167 - accuracy: 1.0000 - 20ms/epoch - 5ms/step\n",
            "Epoch 141/150\n",
            "4/4 - 0s - loss: 0.0149 - accuracy: 1.0000 - 23ms/epoch - 6ms/step\n",
            "Epoch 142/150\n",
            "4/4 - 0s - loss: 0.0134 - accuracy: 1.0000 - 14ms/epoch - 3ms/step\n",
            "Epoch 143/150\n",
            "4/4 - 0s - loss: 0.0128 - accuracy: 1.0000 - 25ms/epoch - 6ms/step\n",
            "Epoch 144/150\n",
            "4/4 - 0s - loss: 0.0115 - accuracy: 1.0000 - 22ms/epoch - 6ms/step\n",
            "Epoch 145/150\n",
            "4/4 - 0s - loss: 0.0112 - accuracy: 1.0000 - 19ms/epoch - 5ms/step\n",
            "Epoch 146/150\n",
            "4/4 - 0s - loss: 0.0105 - accuracy: 1.0000 - 15ms/epoch - 4ms/step\n",
            "Epoch 147/150\n",
            "4/4 - 0s - loss: 0.0098 - accuracy: 1.0000 - 21ms/epoch - 5ms/step\n",
            "Epoch 148/150\n",
            "4/4 - 0s - loss: 0.0094 - accuracy: 1.0000 - 16ms/epoch - 4ms/step\n",
            "Epoch 149/150\n",
            "4/4 - 0s - loss: 0.0088 - accuracy: 1.0000 - 29ms/epoch - 7ms/step\n",
            "Epoch 150/150\n",
            "4/4 - 0s - loss: 0.0085 - accuracy: 1.0000 - 14ms/epoch - 4ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Final Predictions:\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2\n",
        "#Hidden Layers: 2\n",
        "#(In my case for using the sklearn it utilizied alpha parameter that directly affects the regularization)\n",
        "#Regularization Lambda: 0\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/car_evaluation.csv')\n",
        "\n",
        "# Encode the categorical target variable\n",
        "le = LabelEncoder()\n",
        "data = data.apply(le.fit_transform)\n",
        "\n",
        "X = data.iloc[:, 0:6]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "X = X.values\n",
        "y = y.values\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create and train the neural network model\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(2,), max_iter=500,alpha = 0, random_state=42, verbose = True)\n",
        "ovr_classifier = OneVsRestClassifier(mlp_classifier)\n",
        "\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions train set\n",
        "y_train_pred = ovr_classifier.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "#Generate Predictions on test set\n",
        "y_test_pred = ovr_classifier.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "\n",
        "#Probablility estimates for each class on the test set\n",
        "y_test_probs = ovr_classifier.predict_proba(X_test)\n",
        "\n",
        "test_log_loss = log_loss(y_test, y_test_probs)\n",
        "\n",
        "\n",
        "\n",
        "print(f'Training Final Percent Accuracy: {train_accuracy * 100:.2f}%')\n",
        "print(f'Testing Final Percent Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Final Log Loss on Test Set: {test_log_loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "q9UbWvE-oxfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2\n",
        "#Hidden Layers: 2\n",
        "#(In my case for using the sklearn it utilizied alpha parameter that directly affects the regularization)\n",
        "#Regularization Lambda: .25\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/car_evaluation.csv')\n",
        "\n",
        "# Encode the categorical target variable\n",
        "le = LabelEncoder()\n",
        "data = data.apply(le.fit_transform)\n",
        "\n",
        "X = data.iloc[:, 0:6]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "X = X.values\n",
        "y = y.values\n",
        "\n",
        "# Separate features and target variable\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create and train the neural network model\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(2,), max_iter=500 ,alpha = 0.25, random_state=42, verbose = True)\n",
        "ovr_classifier = OneVsRestClassifier(mlp_classifier)\n",
        "\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the train set\n",
        "y_train_pred = ovr_classifier.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "#Predictions on the test set\n",
        "y_test_pred = ovr_classifier.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "\n",
        "#Probablility estimates for each class on the test set\n",
        "y_test_probs = ovr_classifier.predict_proba(X_test)\n",
        "\n",
        "test_log_loss = log_loss(y_test, y_test_probs)\n",
        "\n",
        "\n",
        "\n",
        "print(f'Training Final Percent Accuracy: {train_accuracy * 100:.2f}%')\n",
        "print(f'Testing Final Percent Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Final Log Loss on Test Set: {test_log_loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "2Qfifd_qL-4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2\n",
        "#Hidden Layers: 6\n",
        "#(In my case for using the sklearn it ustilizied alpha parameter that directly affects the regullarization)\n",
        "#Regularization Lambda: 0\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/car_evaluation.csv')\n",
        "\n",
        "# Encode the categorical target variable\n",
        "le = LabelEncoder()\n",
        "data = data.apply(le.fit_transform)\n",
        "\n",
        "X = data.iloc[:, 0:6]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "X = X.values\n",
        "y = y.values\n",
        "\n",
        "# Separate features and target variable\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create and train the neural network model\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(6,), max_iter=500 ,alpha = 0, random_state=42, verbose = True)\n",
        "ovr_classifier = OneVsRestClassifier(mlp_classifier)\n",
        "\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the train set\n",
        "y_train_pred = ovr_classifier.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "#Predictions on the test set\n",
        "y_test_pred = ovr_classifier.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "\n",
        "#Probablility estimates for each class on the test set\n",
        "y_test_probs = ovr_classifier.predict_proba(X_test)\n",
        "\n",
        "test_log_loss = log_loss(y_test, y_test_probs)\n",
        "\n",
        "\n",
        "\n",
        "print(f'Training Final Percent Accuracy: {train_accuracy * 100:.2f}%')\n",
        "print(f'Testing Final Percent Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Final Log Loss on Test Set: {test_log_loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vaaiERq-rYQ",
        "outputId": "7e52b082-4d99-4241-bdf4-d995a9677881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.94727893\n",
            "Iteration 2, loss = 0.91652910\n",
            "Iteration 3, loss = 0.88804941\n",
            "Iteration 4, loss = 0.86153963\n",
            "Iteration 5, loss = 0.83587897\n",
            "Iteration 6, loss = 0.81278155\n",
            "Iteration 7, loss = 0.79097165\n",
            "Iteration 8, loss = 0.77046672\n",
            "Iteration 9, loss = 0.75124213\n",
            "Iteration 10, loss = 0.73236728\n",
            "Iteration 11, loss = 0.71468375\n",
            "Iteration 12, loss = 0.69776686\n",
            "Iteration 13, loss = 0.68156402\n",
            "Iteration 14, loss = 0.66677762\n",
            "Iteration 15, loss = 0.65307768\n",
            "Iteration 16, loss = 0.64014773\n",
            "Iteration 17, loss = 0.62835747\n",
            "Iteration 18, loss = 0.61737246\n",
            "Iteration 19, loss = 0.60776674\n",
            "Iteration 20, loss = 0.59859284\n",
            "Iteration 21, loss = 0.59132195\n",
            "Iteration 22, loss = 0.58446878\n",
            "Iteration 23, loss = 0.57911262\n",
            "Iteration 24, loss = 0.57378139\n",
            "Iteration 25, loss = 0.56906122\n",
            "Iteration 26, loss = 0.56494967\n",
            "Iteration 27, loss = 0.56112214\n",
            "Iteration 28, loss = 0.55748462\n",
            "Iteration 29, loss = 0.55405007\n",
            "Iteration 30, loss = 0.55080903\n",
            "Iteration 31, loss = 0.54771240\n",
            "Iteration 32, loss = 0.54478610\n",
            "Iteration 33, loss = 0.54210390\n",
            "Iteration 34, loss = 0.53935937\n",
            "Iteration 35, loss = 0.53677849\n",
            "Iteration 36, loss = 0.53433914\n",
            "Iteration 37, loss = 0.53200759\n",
            "Iteration 38, loss = 0.52974666\n",
            "Iteration 39, loss = 0.52746485\n",
            "Iteration 40, loss = 0.52521499\n",
            "Iteration 41, loss = 0.52314188\n",
            "Iteration 42, loss = 0.52118019\n",
            "Iteration 43, loss = 0.51920244\n",
            "Iteration 44, loss = 0.51730796\n",
            "Iteration 45, loss = 0.51531528\n",
            "Iteration 46, loss = 0.51361843\n",
            "Iteration 47, loss = 0.51163783\n",
            "Iteration 48, loss = 0.50976284\n",
            "Iteration 49, loss = 0.50795239\n",
            "Iteration 50, loss = 0.50621892\n",
            "Iteration 51, loss = 0.50441127\n",
            "Iteration 52, loss = 0.50289733\n",
            "Iteration 53, loss = 0.50121183\n",
            "Iteration 54, loss = 0.49964252\n",
            "Iteration 55, loss = 0.49811331\n",
            "Iteration 56, loss = 0.49659929\n",
            "Iteration 57, loss = 0.49498947\n",
            "Iteration 58, loss = 0.49346928\n",
            "Iteration 59, loss = 0.49208286\n",
            "Iteration 60, loss = 0.49072754\n",
            "Iteration 61, loss = 0.48940594\n",
            "Iteration 62, loss = 0.48807445\n",
            "Iteration 63, loss = 0.48690710\n",
            "Iteration 64, loss = 0.48578962\n",
            "Iteration 65, loss = 0.48468571\n",
            "Iteration 66, loss = 0.48364710\n",
            "Iteration 67, loss = 0.48264385\n",
            "Iteration 68, loss = 0.48162816\n",
            "Iteration 69, loss = 0.48067096\n",
            "Iteration 70, loss = 0.47976098\n",
            "Iteration 71, loss = 0.47890597\n",
            "Iteration 72, loss = 0.47803256\n",
            "Iteration 73, loss = 0.47730735\n",
            "Iteration 74, loss = 0.47647559\n",
            "Iteration 75, loss = 0.47572684\n",
            "Iteration 76, loss = 0.47515648\n",
            "Iteration 77, loss = 0.47430486\n",
            "Iteration 78, loss = 0.47353460\n",
            "Iteration 79, loss = 0.47284270\n",
            "Iteration 80, loss = 0.47205335\n",
            "Iteration 81, loss = 0.47140127\n",
            "Iteration 82, loss = 0.47072348\n",
            "Iteration 83, loss = 0.47006665\n",
            "Iteration 84, loss = 0.46944736\n",
            "Iteration 85, loss = 0.46885019\n",
            "Iteration 86, loss = 0.46829059\n",
            "Iteration 87, loss = 0.46773478\n",
            "Iteration 88, loss = 0.46714394\n",
            "Iteration 89, loss = 0.46668210\n",
            "Iteration 90, loss = 0.46601028\n",
            "Iteration 91, loss = 0.46551489\n",
            "Iteration 92, loss = 0.46499755\n",
            "Iteration 93, loss = 0.46457923\n",
            "Iteration 94, loss = 0.46405173\n",
            "Iteration 95, loss = 0.46354070\n",
            "Iteration 96, loss = 0.46311302\n",
            "Iteration 97, loss = 0.46262534\n",
            "Iteration 98, loss = 0.46211564\n",
            "Iteration 99, loss = 0.46175837\n",
            "Iteration 100, loss = 0.46125070\n",
            "Iteration 101, loss = 0.46086645\n",
            "Iteration 102, loss = 0.46050987\n",
            "Iteration 103, loss = 0.46008638\n",
            "Iteration 104, loss = 0.45975262\n",
            "Iteration 105, loss = 0.45936619\n",
            "Iteration 106, loss = 0.45902087\n",
            "Iteration 107, loss = 0.45864645\n",
            "Iteration 108, loss = 0.45833353\n",
            "Iteration 109, loss = 0.45801252\n",
            "Iteration 110, loss = 0.45767238\n",
            "Iteration 111, loss = 0.45738974\n",
            "Iteration 112, loss = 0.45706498\n",
            "Iteration 113, loss = 0.45679716\n",
            "Iteration 114, loss = 0.45645062\n",
            "Iteration 115, loss = 0.45614689\n",
            "Iteration 116, loss = 0.45588862\n",
            "Iteration 117, loss = 0.45557442\n",
            "Iteration 118, loss = 0.45520250\n",
            "Iteration 119, loss = 0.45491377\n",
            "Iteration 120, loss = 0.45459993\n",
            "Iteration 121, loss = 0.45426408\n",
            "Iteration 122, loss = 0.45395818\n",
            "Iteration 123, loss = 0.45362813\n",
            "Iteration 124, loss = 0.45328777\n",
            "Iteration 125, loss = 0.45298105\n",
            "Iteration 126, loss = 0.45260537\n",
            "Iteration 127, loss = 0.45231122\n",
            "Iteration 128, loss = 0.45200862\n",
            "Iteration 129, loss = 0.45169206\n",
            "Iteration 130, loss = 0.45135323\n",
            "Iteration 131, loss = 0.45110432\n",
            "Iteration 132, loss = 0.45076577\n",
            "Iteration 133, loss = 0.45044707\n",
            "Iteration 134, loss = 0.45014394\n",
            "Iteration 135, loss = 0.44981221\n",
            "Iteration 136, loss = 0.44954681\n",
            "Iteration 137, loss = 0.44923676\n",
            "Iteration 138, loss = 0.44895641\n",
            "Iteration 139, loss = 0.44870797\n",
            "Iteration 140, loss = 0.44837379\n",
            "Iteration 141, loss = 0.44808081\n",
            "Iteration 142, loss = 0.44781413\n",
            "Iteration 143, loss = 0.44746602\n",
            "Iteration 144, loss = 0.44722529\n",
            "Iteration 145, loss = 0.44686182\n",
            "Iteration 146, loss = 0.44657628\n",
            "Iteration 147, loss = 0.44624310\n",
            "Iteration 148, loss = 0.44594244\n",
            "Iteration 149, loss = 0.44556444\n",
            "Iteration 150, loss = 0.44520560\n",
            "Iteration 151, loss = 0.44487716\n",
            "Iteration 152, loss = 0.44452923\n",
            "Iteration 153, loss = 0.44419122\n",
            "Iteration 154, loss = 0.44386752\n",
            "Iteration 155, loss = 0.44347798\n",
            "Iteration 156, loss = 0.44308956\n",
            "Iteration 157, loss = 0.44277875\n",
            "Iteration 158, loss = 0.44238217\n",
            "Iteration 159, loss = 0.44201054\n",
            "Iteration 160, loss = 0.44159440\n",
            "Iteration 161, loss = 0.44129421\n",
            "Iteration 162, loss = 0.44080552\n",
            "Iteration 163, loss = 0.44042437\n",
            "Iteration 164, loss = 0.44003782\n",
            "Iteration 165, loss = 0.43965450\n",
            "Iteration 166, loss = 0.43926211\n",
            "Iteration 167, loss = 0.43880258\n",
            "Iteration 168, loss = 0.43851319\n",
            "Iteration 169, loss = 0.43802832\n",
            "Iteration 170, loss = 0.43762752\n",
            "Iteration 171, loss = 0.43724731\n",
            "Iteration 172, loss = 0.43687467\n",
            "Iteration 173, loss = 0.43650637\n",
            "Iteration 174, loss = 0.43613527\n",
            "Iteration 175, loss = 0.43575365\n",
            "Iteration 176, loss = 0.43539913\n",
            "Iteration 177, loss = 0.43499981\n",
            "Iteration 178, loss = 0.43466322\n",
            "Iteration 179, loss = 0.43423201\n",
            "Iteration 180, loss = 0.43380509\n",
            "Iteration 181, loss = 0.43344642\n",
            "Iteration 182, loss = 0.43304035\n",
            "Iteration 183, loss = 0.43271231\n",
            "Iteration 184, loss = 0.43234733\n",
            "Iteration 185, loss = 0.43192349\n",
            "Iteration 186, loss = 0.43147117\n",
            "Iteration 187, loss = 0.43116925\n",
            "Iteration 188, loss = 0.43073136\n",
            "Iteration 189, loss = 0.43041440\n",
            "Iteration 190, loss = 0.43003405\n",
            "Iteration 191, loss = 0.42971085\n",
            "Iteration 192, loss = 0.42935746\n",
            "Iteration 193, loss = 0.42902874\n",
            "Iteration 194, loss = 0.42870362\n",
            "Iteration 195, loss = 0.42839878\n",
            "Iteration 196, loss = 0.42799196\n",
            "Iteration 197, loss = 0.42768497\n",
            "Iteration 198, loss = 0.42738955\n",
            "Iteration 199, loss = 0.42706566\n",
            "Iteration 200, loss = 0.42677378\n",
            "Iteration 201, loss = 0.42641488\n",
            "Iteration 202, loss = 0.42617177\n",
            "Iteration 203, loss = 0.42578574\n",
            "Iteration 204, loss = 0.42549185\n",
            "Iteration 205, loss = 0.42517834\n",
            "Iteration 206, loss = 0.42489463\n",
            "Iteration 207, loss = 0.42459009\n",
            "Iteration 208, loss = 0.42425971\n",
            "Iteration 209, loss = 0.42394874\n",
            "Iteration 210, loss = 0.42363588\n",
            "Iteration 211, loss = 0.42334190\n",
            "Iteration 212, loss = 0.42304329\n",
            "Iteration 213, loss = 0.42270537\n",
            "Iteration 214, loss = 0.42235390\n",
            "Iteration 215, loss = 0.42200360\n",
            "Iteration 216, loss = 0.42174266\n",
            "Iteration 217, loss = 0.42141492\n",
            "Iteration 218, loss = 0.42117059\n",
            "Iteration 219, loss = 0.42088776\n",
            "Iteration 220, loss = 0.42051822\n",
            "Iteration 221, loss = 0.42023799\n",
            "Iteration 222, loss = 0.41999410\n",
            "Iteration 223, loss = 0.41968808\n",
            "Iteration 224, loss = 0.41938554\n",
            "Iteration 225, loss = 0.41911094\n",
            "Iteration 226, loss = 0.41889996\n",
            "Iteration 227, loss = 0.41856755\n",
            "Iteration 228, loss = 0.41831963\n",
            "Iteration 229, loss = 0.41806729\n",
            "Iteration 230, loss = 0.41777895\n",
            "Iteration 231, loss = 0.41752024\n",
            "Iteration 232, loss = 0.41728862\n",
            "Iteration 233, loss = 0.41703376\n",
            "Iteration 234, loss = 0.41682002\n",
            "Iteration 235, loss = 0.41650062\n",
            "Iteration 236, loss = 0.41624165\n",
            "Iteration 237, loss = 0.41599991\n",
            "Iteration 238, loss = 0.41573289\n",
            "Iteration 239, loss = 0.41550649\n",
            "Iteration 240, loss = 0.41526744\n",
            "Iteration 241, loss = 0.41504094\n",
            "Iteration 242, loss = 0.41476136\n",
            "Iteration 243, loss = 0.41451407\n",
            "Iteration 244, loss = 0.41422965\n",
            "Iteration 245, loss = 0.41401800\n",
            "Iteration 246, loss = 0.41375029\n",
            "Iteration 247, loss = 0.41361187\n",
            "Iteration 248, loss = 0.41327364\n",
            "Iteration 249, loss = 0.41309614\n",
            "Iteration 250, loss = 0.41283786\n",
            "Iteration 251, loss = 0.41258453\n",
            "Iteration 252, loss = 0.41238601\n",
            "Iteration 253, loss = 0.41215635\n",
            "Iteration 254, loss = 0.41190592\n",
            "Iteration 255, loss = 0.41172293\n",
            "Iteration 256, loss = 0.41151950\n",
            "Iteration 257, loss = 0.41128917\n",
            "Iteration 258, loss = 0.41108144\n",
            "Iteration 259, loss = 0.41086509\n",
            "Iteration 260, loss = 0.41068610\n",
            "Iteration 261, loss = 0.41043424\n",
            "Iteration 262, loss = 0.41028300\n",
            "Iteration 263, loss = 0.41010273\n",
            "Iteration 264, loss = 0.40984845\n",
            "Iteration 265, loss = 0.40961288\n",
            "Iteration 266, loss = 0.40937423\n",
            "Iteration 267, loss = 0.40918070\n",
            "Iteration 268, loss = 0.40893259\n",
            "Iteration 269, loss = 0.40874412\n",
            "Iteration 270, loss = 0.40854445\n",
            "Iteration 271, loss = 0.40827231\n",
            "Iteration 272, loss = 0.40801831\n",
            "Iteration 273, loss = 0.40782161\n",
            "Iteration 274, loss = 0.40754612\n",
            "Iteration 275, loss = 0.40729145\n",
            "Iteration 276, loss = 0.40699917\n",
            "Iteration 277, loss = 0.40673551\n",
            "Iteration 278, loss = 0.40642270\n",
            "Iteration 279, loss = 0.40616529\n",
            "Iteration 280, loss = 0.40594297\n",
            "Iteration 281, loss = 0.40555529\n",
            "Iteration 282, loss = 0.40522354\n",
            "Iteration 283, loss = 0.40485191\n",
            "Iteration 284, loss = 0.40447521\n",
            "Iteration 285, loss = 0.40412289\n",
            "Iteration 286, loss = 0.40371687\n",
            "Iteration 287, loss = 0.40333112\n",
            "Iteration 288, loss = 0.40287575\n",
            "Iteration 289, loss = 0.40245884\n",
            "Iteration 290, loss = 0.40211919\n",
            "Iteration 291, loss = 0.40161808\n",
            "Iteration 292, loss = 0.40119895\n",
            "Iteration 293, loss = 0.40076380\n",
            "Iteration 294, loss = 0.40035335\n",
            "Iteration 295, loss = 0.39986348\n",
            "Iteration 296, loss = 0.39953240\n",
            "Iteration 297, loss = 0.39919490\n",
            "Iteration 298, loss = 0.39862150\n",
            "Iteration 299, loss = 0.39822759\n",
            "Iteration 300, loss = 0.39777627\n",
            "Iteration 301, loss = 0.39736464\n",
            "Iteration 302, loss = 0.39693764\n",
            "Iteration 303, loss = 0.39651083\n",
            "Iteration 304, loss = 0.39616615\n",
            "Iteration 305, loss = 0.39561521\n",
            "Iteration 306, loss = 0.39526234\n",
            "Iteration 307, loss = 0.39478002\n",
            "Iteration 308, loss = 0.39439247\n",
            "Iteration 309, loss = 0.39392294\n",
            "Iteration 310, loss = 0.39349662\n",
            "Iteration 311, loss = 0.39307815\n",
            "Iteration 312, loss = 0.39264253\n",
            "Iteration 313, loss = 0.39218072\n",
            "Iteration 314, loss = 0.39174555\n",
            "Iteration 315, loss = 0.39139498\n",
            "Iteration 316, loss = 0.39087442\n",
            "Iteration 317, loss = 0.39040499\n",
            "Iteration 318, loss = 0.39001891\n",
            "Iteration 319, loss = 0.38952827\n",
            "Iteration 320, loss = 0.38909062\n",
            "Iteration 321, loss = 0.38868799\n",
            "Iteration 322, loss = 0.38825125\n",
            "Iteration 323, loss = 0.38786037\n",
            "Iteration 324, loss = 0.38724080\n",
            "Iteration 325, loss = 0.38679024\n",
            "Iteration 326, loss = 0.38633151\n",
            "Iteration 327, loss = 0.38582876\n",
            "Iteration 328, loss = 0.38542464\n",
            "Iteration 329, loss = 0.38484020\n",
            "Iteration 330, loss = 0.38434034\n",
            "Iteration 331, loss = 0.38408216\n",
            "Iteration 332, loss = 0.38337655\n",
            "Iteration 333, loss = 0.38280325\n",
            "Iteration 334, loss = 0.38228472\n",
            "Iteration 335, loss = 0.38175779\n",
            "Iteration 336, loss = 0.38129442\n",
            "Iteration 337, loss = 0.38083225\n",
            "Iteration 338, loss = 0.38035741\n",
            "Iteration 339, loss = 0.37966339\n",
            "Iteration 340, loss = 0.37920544\n",
            "Iteration 341, loss = 0.37877578\n",
            "Iteration 342, loss = 0.37819109\n",
            "Iteration 343, loss = 0.37773506\n",
            "Iteration 344, loss = 0.37722095\n",
            "Iteration 345, loss = 0.37684595\n",
            "Iteration 346, loss = 0.37617782\n",
            "Iteration 347, loss = 0.37575587\n",
            "Iteration 348, loss = 0.37519959\n",
            "Iteration 349, loss = 0.37470060\n",
            "Iteration 350, loss = 0.37416903\n",
            "Iteration 351, loss = 0.37361682\n",
            "Iteration 352, loss = 0.37303494\n",
            "Iteration 353, loss = 0.37251630\n",
            "Iteration 354, loss = 0.37179737\n",
            "Iteration 355, loss = 0.37120263\n",
            "Iteration 356, loss = 0.37062427\n",
            "Iteration 357, loss = 0.37001399\n",
            "Iteration 358, loss = 0.36938148\n",
            "Iteration 359, loss = 0.36878171\n",
            "Iteration 360, loss = 0.36820011\n",
            "Iteration 361, loss = 0.36769945\n",
            "Iteration 362, loss = 0.36720785\n",
            "Iteration 363, loss = 0.36652417\n",
            "Iteration 364, loss = 0.36591889\n",
            "Iteration 365, loss = 0.36542552\n",
            "Iteration 366, loss = 0.36485332\n",
            "Iteration 367, loss = 0.36436254\n",
            "Iteration 368, loss = 0.36371463\n",
            "Iteration 369, loss = 0.36328341\n",
            "Iteration 370, loss = 0.36277772\n",
            "Iteration 371, loss = 0.36218947\n",
            "Iteration 372, loss = 0.36166406\n",
            "Iteration 373, loss = 0.36115445\n",
            "Iteration 374, loss = 0.36061099\n",
            "Iteration 375, loss = 0.36017478\n",
            "Iteration 376, loss = 0.35967345\n",
            "Iteration 377, loss = 0.35925472\n",
            "Iteration 378, loss = 0.35870965\n",
            "Iteration 379, loss = 0.35815621\n",
            "Iteration 380, loss = 0.35770263\n",
            "Iteration 381, loss = 0.35709891\n",
            "Iteration 382, loss = 0.35656778\n",
            "Iteration 383, loss = 0.35609414\n",
            "Iteration 384, loss = 0.35555769\n",
            "Iteration 385, loss = 0.35506697\n",
            "Iteration 386, loss = 0.35458926\n",
            "Iteration 387, loss = 0.35401068\n",
            "Iteration 388, loss = 0.35343837\n",
            "Iteration 389, loss = 0.35293817\n",
            "Iteration 390, loss = 0.35241759\n",
            "Iteration 391, loss = 0.35192636\n",
            "Iteration 392, loss = 0.35155492\n",
            "Iteration 393, loss = 0.35084588\n",
            "Iteration 394, loss = 0.35043099\n",
            "Iteration 395, loss = 0.34995116\n",
            "Iteration 396, loss = 0.34942661\n",
            "Iteration 397, loss = 0.34900438\n",
            "Iteration 398, loss = 0.34838734\n",
            "Iteration 399, loss = 0.34791802\n",
            "Iteration 400, loss = 0.34739854\n",
            "Iteration 401, loss = 0.34692037\n",
            "Iteration 402, loss = 0.34649119\n",
            "Iteration 403, loss = 0.34591717\n",
            "Iteration 404, loss = 0.34557637\n",
            "Iteration 405, loss = 0.34498452\n",
            "Iteration 406, loss = 0.34466679\n",
            "Iteration 407, loss = 0.34404199\n",
            "Iteration 408, loss = 0.34361552\n",
            "Iteration 409, loss = 0.34320168\n",
            "Iteration 410, loss = 0.34268020\n",
            "Iteration 411, loss = 0.34224889\n",
            "Iteration 412, loss = 0.34178921\n",
            "Iteration 413, loss = 0.34127553\n",
            "Iteration 414, loss = 0.34088913\n",
            "Iteration 415, loss = 0.34043627\n",
            "Iteration 416, loss = 0.33999920\n",
            "Iteration 417, loss = 0.33949020\n",
            "Iteration 418, loss = 0.33898499\n",
            "Iteration 419, loss = 0.33867884\n",
            "Iteration 420, loss = 0.33842155\n",
            "Iteration 421, loss = 0.33763003\n",
            "Iteration 422, loss = 0.33720332\n",
            "Iteration 423, loss = 0.33676224\n",
            "Iteration 424, loss = 0.33637961\n",
            "Iteration 425, loss = 0.33591343\n",
            "Iteration 426, loss = 0.33546280\n",
            "Iteration 427, loss = 0.33504949\n",
            "Iteration 428, loss = 0.33457579\n",
            "Iteration 429, loss = 0.33420213\n",
            "Iteration 430, loss = 0.33375592\n",
            "Iteration 431, loss = 0.33346623\n",
            "Iteration 432, loss = 0.33299179\n",
            "Iteration 433, loss = 0.33262963\n",
            "Iteration 434, loss = 0.33218454\n",
            "Iteration 435, loss = 0.33173261\n",
            "Iteration 436, loss = 0.33142532\n",
            "Iteration 437, loss = 0.33106668\n",
            "Iteration 438, loss = 0.33060253\n",
            "Iteration 439, loss = 0.33022788\n",
            "Iteration 440, loss = 0.32978469\n",
            "Iteration 441, loss = 0.32941059\n",
            "Iteration 442, loss = 0.32911216\n",
            "Iteration 443, loss = 0.32875379\n",
            "Iteration 444, loss = 0.32833796\n",
            "Iteration 445, loss = 0.32804013\n",
            "Iteration 446, loss = 0.32766631\n",
            "Iteration 447, loss = 0.32728721\n",
            "Iteration 448, loss = 0.32694445\n",
            "Iteration 449, loss = 0.32664789\n",
            "Iteration 450, loss = 0.32629932\n",
            "Iteration 451, loss = 0.32634789\n",
            "Iteration 452, loss = 0.32558268\n",
            "Iteration 453, loss = 0.32552287\n",
            "Iteration 454, loss = 0.32500605\n",
            "Iteration 455, loss = 0.32472988\n",
            "Iteration 456, loss = 0.32426317\n",
            "Iteration 457, loss = 0.32406044\n",
            "Iteration 458, loss = 0.32369091\n",
            "Iteration 459, loss = 0.32338006\n",
            "Iteration 460, loss = 0.32293615\n",
            "Iteration 461, loss = 0.32268614\n",
            "Iteration 462, loss = 0.32239024\n",
            "Iteration 463, loss = 0.32210172\n",
            "Iteration 464, loss = 0.32180088\n",
            "Iteration 465, loss = 0.32154970\n",
            "Iteration 466, loss = 0.32114429\n",
            "Iteration 467, loss = 0.32082654\n",
            "Iteration 468, loss = 0.32060161\n",
            "Iteration 469, loss = 0.32030634\n",
            "Iteration 470, loss = 0.31995850\n",
            "Iteration 471, loss = 0.31964805\n",
            "Iteration 472, loss = 0.31944185\n",
            "Iteration 473, loss = 0.31922461\n",
            "Iteration 474, loss = 0.31881142\n",
            "Iteration 475, loss = 0.31850677\n",
            "Iteration 476, loss = 0.31827416\n",
            "Iteration 477, loss = 0.31816572\n",
            "Iteration 478, loss = 0.31774511\n",
            "Iteration 479, loss = 0.31751518\n",
            "Iteration 480, loss = 0.31719639\n",
            "Iteration 481, loss = 0.31693946\n",
            "Iteration 482, loss = 0.31696628\n",
            "Iteration 483, loss = 0.31653830\n",
            "Iteration 484, loss = 0.31619690\n",
            "Iteration 485, loss = 0.31598918\n",
            "Iteration 486, loss = 0.31569283\n",
            "Iteration 487, loss = 0.31550068\n",
            "Iteration 488, loss = 0.31516815\n",
            "Iteration 489, loss = 0.31502572\n",
            "Iteration 490, loss = 0.31474215\n",
            "Iteration 491, loss = 0.31446233\n",
            "Iteration 492, loss = 0.31420970\n",
            "Iteration 493, loss = 0.31406599\n",
            "Iteration 494, loss = 0.31372031\n",
            "Iteration 495, loss = 0.31350597\n",
            "Iteration 496, loss = 0.31330599\n",
            "Iteration 497, loss = 0.31306229\n",
            "Iteration 498, loss = 0.31276548\n",
            "Iteration 499, loss = 0.31253946\n",
            "Iteration 500, loss = 0.31236102\n",
            "Iteration 1, loss = 0.99933110\n",
            "Iteration 2, loss = 0.95676132\n",
            "Iteration 3, loss = 0.91601288\n",
            "Iteration 4, loss = 0.87727038\n",
            "Iteration 5, loss = 0.83993569\n",
            "Iteration 6, loss = 0.80398799\n",
            "Iteration 7, loss = 0.76871386\n",
            "Iteration 8, loss = 0.73432375\n",
            "Iteration 9, loss = 0.70032850\n",
            "Iteration 10, loss = 0.66629860\n",
            "Iteration 11, loss = 0.63292790\n",
            "Iteration 12, loss = 0.59835380\n",
            "Iteration 13, loss = 0.56440801\n",
            "Iteration 14, loss = 0.53105098\n",
            "Iteration 15, loss = 0.49874579\n",
            "Iteration 16, loss = 0.46713930\n",
            "Iteration 17, loss = 0.43678983\n",
            "Iteration 18, loss = 0.40819061\n",
            "Iteration 19, loss = 0.38154746\n",
            "Iteration 20, loss = 0.35824572\n",
            "Iteration 21, loss = 0.33675309\n",
            "Iteration 22, loss = 0.31782412\n",
            "Iteration 23, loss = 0.30100748\n",
            "Iteration 24, loss = 0.28636969\n",
            "Iteration 25, loss = 0.27402454\n",
            "Iteration 26, loss = 0.26280296\n",
            "Iteration 27, loss = 0.25379687\n",
            "Iteration 28, loss = 0.24589692\n",
            "Iteration 29, loss = 0.23919188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 30, loss = 0.23355130\n",
            "Iteration 31, loss = 0.22864492\n",
            "Iteration 32, loss = 0.22442143\n",
            "Iteration 33, loss = 0.22092998\n",
            "Iteration 34, loss = 0.21760418\n",
            "Iteration 35, loss = 0.21489446\n",
            "Iteration 36, loss = 0.21251197\n",
            "Iteration 37, loss = 0.21048443\n",
            "Iteration 38, loss = 0.20855333\n",
            "Iteration 39, loss = 0.20680116\n",
            "Iteration 40, loss = 0.20534217\n",
            "Iteration 41, loss = 0.20391781\n",
            "Iteration 42, loss = 0.20260443\n",
            "Iteration 43, loss = 0.20137418\n",
            "Iteration 44, loss = 0.20017842\n",
            "Iteration 45, loss = 0.19915294\n",
            "Iteration 46, loss = 0.19811041\n",
            "Iteration 47, loss = 0.19717927\n",
            "Iteration 48, loss = 0.19615263\n",
            "Iteration 49, loss = 0.19522310\n",
            "Iteration 50, loss = 0.19434277\n",
            "Iteration 51, loss = 0.19347155\n",
            "Iteration 52, loss = 0.19263631\n",
            "Iteration 53, loss = 0.19183018\n",
            "Iteration 54, loss = 0.19105821\n",
            "Iteration 55, loss = 0.19032650\n",
            "Iteration 56, loss = 0.18961066\n",
            "Iteration 57, loss = 0.18893540\n",
            "Iteration 58, loss = 0.18827778\n",
            "Iteration 59, loss = 0.18765750\n",
            "Iteration 60, loss = 0.18703081\n",
            "Iteration 61, loss = 0.18646860\n",
            "Iteration 62, loss = 0.18586394\n",
            "Iteration 63, loss = 0.18530226\n",
            "Iteration 64, loss = 0.18469735\n",
            "Iteration 65, loss = 0.18417065\n",
            "Iteration 66, loss = 0.18360097\n",
            "Iteration 67, loss = 0.18310665\n",
            "Iteration 68, loss = 0.18258101\n",
            "Iteration 69, loss = 0.18205447\n",
            "Iteration 70, loss = 0.18159069\n",
            "Iteration 71, loss = 0.18108771\n",
            "Iteration 72, loss = 0.18062730\n",
            "Iteration 73, loss = 0.18013955\n",
            "Iteration 74, loss = 0.17972933\n",
            "Iteration 75, loss = 0.17921702\n",
            "Iteration 76, loss = 0.17877046\n",
            "Iteration 77, loss = 0.17837291\n",
            "Iteration 78, loss = 0.17787036\n",
            "Iteration 79, loss = 0.17741562\n",
            "Iteration 80, loss = 0.17701157\n",
            "Iteration 81, loss = 0.17662996\n",
            "Iteration 82, loss = 0.17617829\n",
            "Iteration 83, loss = 0.17571587\n",
            "Iteration 84, loss = 0.17528210\n",
            "Iteration 85, loss = 0.17484055\n",
            "Iteration 86, loss = 0.17442650\n",
            "Iteration 87, loss = 0.17403255\n",
            "Iteration 88, loss = 0.17361332\n",
            "Iteration 89, loss = 0.17321890\n",
            "Iteration 90, loss = 0.17280741\n",
            "Iteration 91, loss = 0.17242828\n",
            "Iteration 92, loss = 0.17203828\n",
            "Iteration 93, loss = 0.17162342\n",
            "Iteration 94, loss = 0.17122454\n",
            "Iteration 95, loss = 0.17083043\n",
            "Iteration 96, loss = 0.17045377\n",
            "Iteration 97, loss = 0.17005378\n",
            "Iteration 98, loss = 0.16971112\n",
            "Iteration 99, loss = 0.16929597\n",
            "Iteration 100, loss = 0.16892909\n",
            "Iteration 101, loss = 0.16855878\n",
            "Iteration 102, loss = 0.16820450\n",
            "Iteration 103, loss = 0.16790479\n",
            "Iteration 104, loss = 0.16756406\n",
            "Iteration 105, loss = 0.16721844\n",
            "Iteration 106, loss = 0.16692411\n",
            "Iteration 107, loss = 0.16660197\n",
            "Iteration 108, loss = 0.16632192\n",
            "Iteration 109, loss = 0.16599498\n",
            "Iteration 110, loss = 0.16569285\n",
            "Iteration 111, loss = 0.16540895\n",
            "Iteration 112, loss = 0.16509231\n",
            "Iteration 113, loss = 0.16479483\n",
            "Iteration 114, loss = 0.16449695\n",
            "Iteration 115, loss = 0.16421559\n",
            "Iteration 116, loss = 0.16390554\n",
            "Iteration 117, loss = 0.16360114\n",
            "Iteration 118, loss = 0.16332373\n",
            "Iteration 119, loss = 0.16300076\n",
            "Iteration 120, loss = 0.16276691\n",
            "Iteration 121, loss = 0.16241212\n",
            "Iteration 122, loss = 0.16213590\n",
            "Iteration 123, loss = 0.16183142\n",
            "Iteration 124, loss = 0.16154358\n",
            "Iteration 125, loss = 0.16124511\n",
            "Iteration 126, loss = 0.16097821\n",
            "Iteration 127, loss = 0.16069773\n",
            "Iteration 128, loss = 0.16036446\n",
            "Iteration 129, loss = 0.16009609\n",
            "Iteration 130, loss = 0.15981310\n",
            "Iteration 131, loss = 0.15953261\n",
            "Iteration 132, loss = 0.15928223\n",
            "Iteration 133, loss = 0.15897026\n",
            "Iteration 134, loss = 0.15871550\n",
            "Iteration 135, loss = 0.15844046\n",
            "Iteration 136, loss = 0.15813364\n",
            "Iteration 137, loss = 0.15785988\n",
            "Iteration 138, loss = 0.15759787\n",
            "Iteration 139, loss = 0.15730119\n",
            "Iteration 140, loss = 0.15704584\n",
            "Iteration 141, loss = 0.15674634\n",
            "Iteration 142, loss = 0.15645342\n",
            "Iteration 143, loss = 0.15619806\n",
            "Iteration 144, loss = 0.15594823\n",
            "Iteration 145, loss = 0.15565781\n",
            "Iteration 146, loss = 0.15538887\n",
            "Iteration 147, loss = 0.15510696\n",
            "Iteration 148, loss = 0.15483593\n",
            "Iteration 149, loss = 0.15457784\n",
            "Iteration 150, loss = 0.15431211\n",
            "Iteration 151, loss = 0.15406477\n",
            "Iteration 152, loss = 0.15379400\n",
            "Iteration 153, loss = 0.15354955\n",
            "Iteration 154, loss = 0.15331443\n",
            "Iteration 155, loss = 0.15313950\n",
            "Iteration 156, loss = 0.15289825\n",
            "Iteration 157, loss = 0.15259645\n",
            "Iteration 158, loss = 0.15237642\n",
            "Iteration 159, loss = 0.15211299\n",
            "Iteration 160, loss = 0.15187975\n",
            "Iteration 161, loss = 0.15163321\n",
            "Iteration 162, loss = 0.15140788\n",
            "Iteration 163, loss = 0.15118423\n",
            "Iteration 164, loss = 0.15094413\n",
            "Iteration 165, loss = 0.15073321\n",
            "Iteration 166, loss = 0.15056558\n",
            "Iteration 167, loss = 0.15027501\n",
            "Iteration 168, loss = 0.15010501\n",
            "Iteration 169, loss = 0.14989605\n",
            "Iteration 170, loss = 0.14965667\n",
            "Iteration 171, loss = 0.14946568\n",
            "Iteration 172, loss = 0.14927334\n",
            "Iteration 173, loss = 0.14906949\n",
            "Iteration 174, loss = 0.14886578\n",
            "Iteration 175, loss = 0.14868863\n",
            "Iteration 176, loss = 0.14849831\n",
            "Iteration 177, loss = 0.14831087\n",
            "Iteration 178, loss = 0.14810453\n",
            "Iteration 179, loss = 0.14791552\n",
            "Iteration 180, loss = 0.14771597\n",
            "Iteration 181, loss = 0.14759173\n",
            "Iteration 182, loss = 0.14734865\n",
            "Iteration 183, loss = 0.14715561\n",
            "Iteration 184, loss = 0.14699493\n",
            "Iteration 185, loss = 0.14679519\n",
            "Iteration 186, loss = 0.14660987\n",
            "Iteration 187, loss = 0.14641531\n",
            "Iteration 188, loss = 0.14629369\n",
            "Iteration 189, loss = 0.14604582\n",
            "Iteration 190, loss = 0.14585979\n",
            "Iteration 191, loss = 0.14567663\n",
            "Iteration 192, loss = 0.14550037\n",
            "Iteration 193, loss = 0.14532046\n",
            "Iteration 194, loss = 0.14522013\n",
            "Iteration 195, loss = 0.14494705\n",
            "Iteration 196, loss = 0.14476550\n",
            "Iteration 197, loss = 0.14458908\n",
            "Iteration 198, loss = 0.14441318\n",
            "Iteration 199, loss = 0.14424699\n",
            "Iteration 200, loss = 0.14407830\n",
            "Iteration 201, loss = 0.14396566\n",
            "Iteration 202, loss = 0.14376636\n",
            "Iteration 203, loss = 0.14359713\n",
            "Iteration 204, loss = 0.14341287\n",
            "Iteration 205, loss = 0.14325455\n",
            "Iteration 206, loss = 0.14311083\n",
            "Iteration 207, loss = 0.14292356\n",
            "Iteration 208, loss = 0.14277959\n",
            "Iteration 209, loss = 0.14260997\n",
            "Iteration 210, loss = 0.14253362\n",
            "Iteration 211, loss = 0.14228734\n",
            "Iteration 212, loss = 0.14218303\n",
            "Iteration 213, loss = 0.14201266\n",
            "Iteration 214, loss = 0.14183131\n",
            "Iteration 215, loss = 0.14167358\n",
            "Iteration 216, loss = 0.14152594\n",
            "Iteration 217, loss = 0.14137558\n",
            "Iteration 218, loss = 0.14126994\n",
            "Iteration 219, loss = 0.14113463\n",
            "Iteration 220, loss = 0.14094627\n",
            "Iteration 221, loss = 0.14080127\n",
            "Iteration 222, loss = 0.14066360\n",
            "Iteration 223, loss = 0.14053593\n",
            "Iteration 224, loss = 0.14039753\n",
            "Iteration 225, loss = 0.14025326\n",
            "Iteration 226, loss = 0.14011301\n",
            "Iteration 227, loss = 0.13996415\n",
            "Iteration 228, loss = 0.13981276\n",
            "Iteration 229, loss = 0.13967207\n",
            "Iteration 230, loss = 0.13955747\n",
            "Iteration 231, loss = 0.13940906\n",
            "Iteration 232, loss = 0.13935801\n",
            "Iteration 233, loss = 0.13913384\n",
            "Iteration 234, loss = 0.13900135\n",
            "Iteration 235, loss = 0.13885967\n",
            "Iteration 236, loss = 0.13876842\n",
            "Iteration 237, loss = 0.13860609\n",
            "Iteration 238, loss = 0.13846748\n",
            "Iteration 239, loss = 0.13846035\n",
            "Iteration 240, loss = 0.13819380\n",
            "Iteration 241, loss = 0.13808177\n",
            "Iteration 242, loss = 0.13792582\n",
            "Iteration 243, loss = 0.13783318\n",
            "Iteration 244, loss = 0.13768990\n",
            "Iteration 245, loss = 0.13754416\n",
            "Iteration 246, loss = 0.13749356\n",
            "Iteration 247, loss = 0.13728628\n",
            "Iteration 248, loss = 0.13716749\n",
            "Iteration 249, loss = 0.13704239\n",
            "Iteration 250, loss = 0.13692296\n",
            "Iteration 251, loss = 0.13678662\n",
            "Iteration 252, loss = 0.13664569\n",
            "Iteration 253, loss = 0.13651832\n",
            "Iteration 254, loss = 0.13640991\n",
            "Iteration 255, loss = 0.13630286\n",
            "Iteration 256, loss = 0.13617329\n",
            "Iteration 257, loss = 0.13605134\n",
            "Iteration 258, loss = 0.13594568\n",
            "Iteration 259, loss = 0.13582172\n",
            "Iteration 260, loss = 0.13569916\n",
            "Iteration 261, loss = 0.13558363\n",
            "Iteration 262, loss = 0.13548108\n",
            "Iteration 263, loss = 0.13535700\n",
            "Iteration 264, loss = 0.13523053\n",
            "Iteration 265, loss = 0.13517691\n",
            "Iteration 266, loss = 0.13502272\n",
            "Iteration 267, loss = 0.13489658\n",
            "Iteration 268, loss = 0.13480953\n",
            "Iteration 269, loss = 0.13474208\n",
            "Iteration 270, loss = 0.13457439\n",
            "Iteration 271, loss = 0.13443525\n",
            "Iteration 272, loss = 0.13433087\n",
            "Iteration 273, loss = 0.13423179\n",
            "Iteration 274, loss = 0.13412837\n",
            "Iteration 275, loss = 0.13403183\n",
            "Iteration 276, loss = 0.13390809\n",
            "Iteration 277, loss = 0.13375855\n",
            "Iteration 278, loss = 0.13367222\n",
            "Iteration 279, loss = 0.13353707\n",
            "Iteration 280, loss = 0.13344652\n",
            "Iteration 281, loss = 0.13339928\n",
            "Iteration 282, loss = 0.13327124\n",
            "Iteration 283, loss = 0.13313341\n",
            "Iteration 284, loss = 0.13302483\n",
            "Iteration 285, loss = 0.13293472\n",
            "Iteration 286, loss = 0.13284834\n",
            "Iteration 287, loss = 0.13272142\n",
            "Iteration 288, loss = 0.13258745\n",
            "Iteration 289, loss = 0.13247811\n",
            "Iteration 290, loss = 0.13237815\n",
            "Iteration 291, loss = 0.13228157\n",
            "Iteration 292, loss = 0.13216718\n",
            "Iteration 293, loss = 0.13209005\n",
            "Iteration 294, loss = 0.13192640\n",
            "Iteration 295, loss = 0.13183588\n",
            "Iteration 296, loss = 0.13179038\n",
            "Iteration 297, loss = 0.13161099\n",
            "Iteration 298, loss = 0.13151697\n",
            "Iteration 299, loss = 0.13140071\n",
            "Iteration 300, loss = 0.13128289\n",
            "Iteration 301, loss = 0.13120094\n",
            "Iteration 302, loss = 0.13108121\n",
            "Iteration 303, loss = 0.13096127\n",
            "Iteration 304, loss = 0.13084492\n",
            "Iteration 305, loss = 0.13072943\n",
            "Iteration 306, loss = 0.13063443\n",
            "Iteration 307, loss = 0.13049701\n",
            "Iteration 308, loss = 0.13040222\n",
            "Iteration 309, loss = 0.13028329\n",
            "Iteration 310, loss = 0.13025529\n",
            "Iteration 311, loss = 0.13006893\n",
            "Iteration 312, loss = 0.12995392\n",
            "Iteration 313, loss = 0.12985833\n",
            "Iteration 314, loss = 0.12973009\n",
            "Iteration 315, loss = 0.12974072\n",
            "Iteration 316, loss = 0.12952508\n",
            "Iteration 317, loss = 0.12939340\n",
            "Iteration 318, loss = 0.12929278\n",
            "Iteration 319, loss = 0.12920484\n",
            "Iteration 320, loss = 0.12906383\n",
            "Iteration 321, loss = 0.12896089\n",
            "Iteration 322, loss = 0.12885461\n",
            "Iteration 323, loss = 0.12871680\n",
            "Iteration 324, loss = 0.12865066\n",
            "Iteration 325, loss = 0.12848544\n",
            "Iteration 326, loss = 0.12837826\n",
            "Iteration 327, loss = 0.12829097\n",
            "Iteration 328, loss = 0.12814309\n",
            "Iteration 329, loss = 0.12806622\n",
            "Iteration 330, loss = 0.12789990\n",
            "Iteration 331, loss = 0.12776003\n",
            "Iteration 332, loss = 0.12763407\n",
            "Iteration 333, loss = 0.12753071\n",
            "Iteration 334, loss = 0.12741418\n",
            "Iteration 335, loss = 0.12731114\n",
            "Iteration 336, loss = 0.12722531\n",
            "Iteration 337, loss = 0.12711339\n",
            "Iteration 338, loss = 0.12697238\n",
            "Iteration 339, loss = 0.12687563\n",
            "Iteration 340, loss = 0.12677177\n",
            "Iteration 341, loss = 0.12668172\n",
            "Iteration 342, loss = 0.12654431\n",
            "Iteration 343, loss = 0.12642824\n",
            "Iteration 344, loss = 0.12633855\n",
            "Iteration 345, loss = 0.12626365\n",
            "Iteration 346, loss = 0.12612502\n",
            "Iteration 347, loss = 0.12601242\n",
            "Iteration 348, loss = 0.12590800\n",
            "Iteration 349, loss = 0.12581657\n",
            "Iteration 350, loss = 0.12571790\n",
            "Iteration 351, loss = 0.12561055\n",
            "Iteration 352, loss = 0.12553064\n",
            "Iteration 353, loss = 0.12541184\n",
            "Iteration 354, loss = 0.12539056\n",
            "Iteration 355, loss = 0.12521887\n",
            "Iteration 356, loss = 0.12516840\n",
            "Iteration 357, loss = 0.12502293\n",
            "Iteration 358, loss = 0.12495502\n",
            "Iteration 359, loss = 0.12483687\n",
            "Iteration 360, loss = 0.12472060\n",
            "Iteration 361, loss = 0.12467336\n",
            "Iteration 362, loss = 0.12459205\n",
            "Iteration 363, loss = 0.12447529\n",
            "Iteration 364, loss = 0.12438818\n",
            "Iteration 365, loss = 0.12435122\n",
            "Iteration 366, loss = 0.12423293\n",
            "Iteration 367, loss = 0.12414891\n",
            "Iteration 368, loss = 0.12406466\n",
            "Iteration 369, loss = 0.12414132\n",
            "Iteration 370, loss = 0.12388713\n",
            "Iteration 371, loss = 0.12385363\n",
            "Iteration 372, loss = 0.12376946\n",
            "Iteration 373, loss = 0.12367310\n",
            "Iteration 374, loss = 0.12357056\n",
            "Iteration 375, loss = 0.12350787\n",
            "Iteration 376, loss = 0.12342859\n",
            "Iteration 377, loss = 0.12335351\n",
            "Iteration 378, loss = 0.12327507\n",
            "Iteration 379, loss = 0.12321671\n",
            "Iteration 380, loss = 0.12311991\n",
            "Iteration 381, loss = 0.12303070\n",
            "Iteration 382, loss = 0.12298932\n",
            "Iteration 383, loss = 0.12285619\n",
            "Iteration 384, loss = 0.12281294\n",
            "Iteration 385, loss = 0.12273288\n",
            "Iteration 386, loss = 0.12266363\n",
            "Iteration 387, loss = 0.12254509\n",
            "Iteration 388, loss = 0.12247842\n",
            "Iteration 389, loss = 0.12236814\n",
            "Iteration 390, loss = 0.12231478\n",
            "Iteration 391, loss = 0.12221740\n",
            "Iteration 392, loss = 0.12213640\n",
            "Iteration 393, loss = 0.12204610\n",
            "Iteration 394, loss = 0.12202914\n",
            "Iteration 395, loss = 0.12192203\n",
            "Iteration 396, loss = 0.12195803\n",
            "Iteration 397, loss = 0.12174113\n",
            "Iteration 398, loss = 0.12164622\n",
            "Iteration 399, loss = 0.12168128\n",
            "Iteration 400, loss = 0.12149557\n",
            "Iteration 401, loss = 0.12142793\n",
            "Iteration 402, loss = 0.12132261\n",
            "Iteration 403, loss = 0.12122863\n",
            "Iteration 404, loss = 0.12115195\n",
            "Iteration 405, loss = 0.12112447\n",
            "Iteration 406, loss = 0.12100502\n",
            "Iteration 407, loss = 0.12088556\n",
            "Iteration 408, loss = 0.12078402\n",
            "Iteration 409, loss = 0.12071871\n",
            "Iteration 410, loss = 0.12064719\n",
            "Iteration 411, loss = 0.12055229\n",
            "Iteration 412, loss = 0.12042547\n",
            "Iteration 413, loss = 0.12034092\n",
            "Iteration 414, loss = 0.12030387\n",
            "Iteration 415, loss = 0.12022155\n",
            "Iteration 416, loss = 0.12009278\n",
            "Iteration 417, loss = 0.12011031\n",
            "Iteration 418, loss = 0.11995183\n",
            "Iteration 419, loss = 0.11982220\n",
            "Iteration 420, loss = 0.11976498\n",
            "Iteration 421, loss = 0.11972682\n",
            "Iteration 422, loss = 0.11959014\n",
            "Iteration 423, loss = 0.11945621\n",
            "Iteration 424, loss = 0.11936041\n",
            "Iteration 425, loss = 0.11928382\n",
            "Iteration 426, loss = 0.11917573\n",
            "Iteration 427, loss = 0.11909675\n",
            "Iteration 428, loss = 0.11900635\n",
            "Iteration 429, loss = 0.11889203\n",
            "Iteration 430, loss = 0.11882017\n",
            "Iteration 431, loss = 0.11873513\n",
            "Iteration 432, loss = 0.11864117\n",
            "Iteration 433, loss = 0.11855836\n",
            "Iteration 434, loss = 0.11844760\n",
            "Iteration 435, loss = 0.11837393\n",
            "Iteration 436, loss = 0.11830020\n",
            "Iteration 437, loss = 0.11828004\n",
            "Iteration 438, loss = 0.11815563\n",
            "Iteration 439, loss = 0.11804239\n",
            "Iteration 440, loss = 0.11798834\n",
            "Iteration 441, loss = 0.11792452\n",
            "Iteration 442, loss = 0.11783999\n",
            "Iteration 443, loss = 0.11776601\n",
            "Iteration 444, loss = 0.11769631\n",
            "Iteration 445, loss = 0.11763323\n",
            "Iteration 446, loss = 0.11757899\n",
            "Iteration 447, loss = 0.11748209\n",
            "Iteration 448, loss = 0.11743622\n",
            "Iteration 449, loss = 0.11738713\n",
            "Iteration 450, loss = 0.11726378\n",
            "Iteration 451, loss = 0.11730945\n",
            "Iteration 452, loss = 0.11714607\n",
            "Iteration 453, loss = 0.11709295\n",
            "Iteration 454, loss = 0.11702675\n",
            "Iteration 455, loss = 0.11696527\n",
            "Iteration 456, loss = 0.11690314\n",
            "Iteration 457, loss = 0.11680795\n",
            "Iteration 458, loss = 0.11674438\n",
            "Iteration 459, loss = 0.11671245\n",
            "Iteration 460, loss = 0.11660675\n",
            "Iteration 461, loss = 0.11656917\n",
            "Iteration 462, loss = 0.11652524\n",
            "Iteration 463, loss = 0.11645998\n",
            "Iteration 464, loss = 0.11637067\n",
            "Iteration 465, loss = 0.11625791\n",
            "Iteration 466, loss = 0.11621308\n",
            "Iteration 467, loss = 0.11611005\n",
            "Iteration 468, loss = 0.11607250\n",
            "Iteration 469, loss = 0.11602364\n",
            "Iteration 470, loss = 0.11595794\n",
            "Iteration 471, loss = 0.11585989\n",
            "Iteration 472, loss = 0.11581696\n",
            "Iteration 473, loss = 0.11572028\n",
            "Iteration 474, loss = 0.11566753\n",
            "Iteration 475, loss = 0.11556304\n",
            "Iteration 476, loss = 0.11548878\n",
            "Iteration 477, loss = 0.11543253\n",
            "Iteration 478, loss = 0.11535774\n",
            "Iteration 479, loss = 0.11530534\n",
            "Iteration 480, loss = 0.11522403\n",
            "Iteration 481, loss = 0.11517924\n",
            "Iteration 482, loss = 0.11510675\n",
            "Iteration 483, loss = 0.11504103\n",
            "Iteration 484, loss = 0.11498136\n",
            "Iteration 485, loss = 0.11496182\n",
            "Iteration 486, loss = 0.11486832\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.60769606\n",
            "Iteration 2, loss = 0.60117270\n",
            "Iteration 3, loss = 0.59578908\n",
            "Iteration 4, loss = 0.59079612\n",
            "Iteration 5, loss = 0.58559836\n",
            "Iteration 6, loss = 0.58142998\n",
            "Iteration 7, loss = 0.57744091\n",
            "Iteration 8, loss = 0.57367858\n",
            "Iteration 9, loss = 0.57028580\n",
            "Iteration 10, loss = 0.56705243\n",
            "Iteration 11, loss = 0.56408241\n",
            "Iteration 12, loss = 0.56126767\n",
            "Iteration 13, loss = 0.55865970\n",
            "Iteration 14, loss = 0.55609979\n",
            "Iteration 15, loss = 0.55349133\n",
            "Iteration 16, loss = 0.55120094\n",
            "Iteration 17, loss = 0.54891679\n",
            "Iteration 18, loss = 0.54695300\n",
            "Iteration 19, loss = 0.54500819\n",
            "Iteration 20, loss = 0.54310315\n",
            "Iteration 21, loss = 0.54138460\n",
            "Iteration 22, loss = 0.53971128\n",
            "Iteration 23, loss = 0.53809911\n",
            "Iteration 24, loss = 0.53648152\n",
            "Iteration 25, loss = 0.53500828\n",
            "Iteration 26, loss = 0.53351859\n",
            "Iteration 27, loss = 0.53218543\n",
            "Iteration 28, loss = 0.53081960\n",
            "Iteration 29, loss = 0.52954120\n",
            "Iteration 30, loss = 0.52831923\n",
            "Iteration 31, loss = 0.52705470\n",
            "Iteration 32, loss = 0.52593990\n",
            "Iteration 33, loss = 0.52471364\n",
            "Iteration 34, loss = 0.52354343\n",
            "Iteration 35, loss = 0.52246651\n",
            "Iteration 36, loss = 0.52132093\n",
            "Iteration 37, loss = 0.52025035\n",
            "Iteration 38, loss = 0.51926384\n",
            "Iteration 39, loss = 0.51800166\n",
            "Iteration 40, loss = 0.51691991\n",
            "Iteration 41, loss = 0.51572971\n",
            "Iteration 42, loss = 0.51468572\n",
            "Iteration 43, loss = 0.51351242\n",
            "Iteration 44, loss = 0.51239315\n",
            "Iteration 45, loss = 0.51124338\n",
            "Iteration 46, loss = 0.51009135\n",
            "Iteration 47, loss = 0.50890840\n",
            "Iteration 48, loss = 0.50770174\n",
            "Iteration 49, loss = 0.50648102\n",
            "Iteration 50, loss = 0.50519700\n",
            "Iteration 51, loss = 0.50389732\n",
            "Iteration 52, loss = 0.50257398\n",
            "Iteration 53, loss = 0.50119414\n",
            "Iteration 54, loss = 0.49990560\n",
            "Iteration 55, loss = 0.49843747\n",
            "Iteration 56, loss = 0.49702290\n",
            "Iteration 57, loss = 0.49542115\n",
            "Iteration 58, loss = 0.49390743\n",
            "Iteration 59, loss = 0.49233286\n",
            "Iteration 60, loss = 0.49076409\n",
            "Iteration 61, loss = 0.48929332\n",
            "Iteration 62, loss = 0.48755548\n",
            "Iteration 63, loss = 0.48595693\n",
            "Iteration 64, loss = 0.48431325\n",
            "Iteration 65, loss = 0.48281809\n",
            "Iteration 66, loss = 0.48120822\n",
            "Iteration 67, loss = 0.47965078\n",
            "Iteration 68, loss = 0.47803626\n",
            "Iteration 69, loss = 0.47654786\n",
            "Iteration 70, loss = 0.47495695\n",
            "Iteration 71, loss = 0.47334848\n",
            "Iteration 72, loss = 0.47187871\n",
            "Iteration 73, loss = 0.47032038\n",
            "Iteration 74, loss = 0.46893917\n",
            "Iteration 75, loss = 0.46735363\n",
            "Iteration 76, loss = 0.46597467\n",
            "Iteration 77, loss = 0.46457490\n",
            "Iteration 78, loss = 0.46310994\n",
            "Iteration 79, loss = 0.46175844\n",
            "Iteration 80, loss = 0.46027860\n",
            "Iteration 81, loss = 0.45892664\n",
            "Iteration 82, loss = 0.45750455\n",
            "Iteration 83, loss = 0.45606596\n",
            "Iteration 84, loss = 0.45465721\n",
            "Iteration 85, loss = 0.45330441\n",
            "Iteration 86, loss = 0.45196083\n",
            "Iteration 87, loss = 0.45061934\n",
            "Iteration 88, loss = 0.44922213\n",
            "Iteration 89, loss = 0.44793335\n",
            "Iteration 90, loss = 0.44654296\n",
            "Iteration 91, loss = 0.44532233\n",
            "Iteration 92, loss = 0.44386929\n",
            "Iteration 93, loss = 0.44253409\n",
            "Iteration 94, loss = 0.44123447\n",
            "Iteration 95, loss = 0.43990246\n",
            "Iteration 96, loss = 0.43863312\n",
            "Iteration 97, loss = 0.43741678\n",
            "Iteration 98, loss = 0.43605340\n",
            "Iteration 99, loss = 0.43481155\n",
            "Iteration 100, loss = 0.43341515\n",
            "Iteration 101, loss = 0.43216456\n",
            "Iteration 102, loss = 0.43093030\n",
            "Iteration 103, loss = 0.42958804\n",
            "Iteration 104, loss = 0.42827885\n",
            "Iteration 105, loss = 0.42699106\n",
            "Iteration 106, loss = 0.42576048\n",
            "Iteration 107, loss = 0.42447619\n",
            "Iteration 108, loss = 0.42326600\n",
            "Iteration 109, loss = 0.42200108\n",
            "Iteration 110, loss = 0.42077313\n",
            "Iteration 111, loss = 0.41957506\n",
            "Iteration 112, loss = 0.41836463\n",
            "Iteration 113, loss = 0.41728237\n",
            "Iteration 114, loss = 0.41604163\n",
            "Iteration 115, loss = 0.41495216\n",
            "Iteration 116, loss = 0.41386236\n",
            "Iteration 117, loss = 0.41287578\n",
            "Iteration 118, loss = 0.41170544\n",
            "Iteration 119, loss = 0.41065471\n",
            "Iteration 120, loss = 0.40968851\n",
            "Iteration 121, loss = 0.40861649\n",
            "Iteration 122, loss = 0.40760621\n",
            "Iteration 123, loss = 0.40661532\n",
            "Iteration 124, loss = 0.40568801\n",
            "Iteration 125, loss = 0.40469907\n",
            "Iteration 126, loss = 0.40375737\n",
            "Iteration 127, loss = 0.40286344\n",
            "Iteration 128, loss = 0.40202548\n",
            "Iteration 129, loss = 0.40119207\n",
            "Iteration 130, loss = 0.40021598\n",
            "Iteration 131, loss = 0.39941386\n",
            "Iteration 132, loss = 0.39853692\n",
            "Iteration 133, loss = 0.39769304\n",
            "Iteration 134, loss = 0.39686048\n",
            "Iteration 135, loss = 0.39606457\n",
            "Iteration 136, loss = 0.39532230\n",
            "Iteration 137, loss = 0.39452226\n",
            "Iteration 138, loss = 0.39371682\n",
            "Iteration 139, loss = 0.39299641\n",
            "Iteration 140, loss = 0.39219887\n",
            "Iteration 141, loss = 0.39140120\n",
            "Iteration 142, loss = 0.39069718\n",
            "Iteration 143, loss = 0.38995398\n",
            "Iteration 144, loss = 0.38917354\n",
            "Iteration 145, loss = 0.38849972\n",
            "Iteration 146, loss = 0.38770106\n",
            "Iteration 147, loss = 0.38701767\n",
            "Iteration 148, loss = 0.38634420\n",
            "Iteration 149, loss = 0.38553370\n",
            "Iteration 150, loss = 0.38489099\n",
            "Iteration 151, loss = 0.38424245\n",
            "Iteration 152, loss = 0.38358994\n",
            "Iteration 153, loss = 0.38288447\n",
            "Iteration 154, loss = 0.38222561\n",
            "Iteration 155, loss = 0.38150406\n",
            "Iteration 156, loss = 0.38088971\n",
            "Iteration 157, loss = 0.38025953\n",
            "Iteration 158, loss = 0.37961832\n",
            "Iteration 159, loss = 0.37898281\n",
            "Iteration 160, loss = 0.37831491\n",
            "Iteration 161, loss = 0.37776279\n",
            "Iteration 162, loss = 0.37702625\n",
            "Iteration 163, loss = 0.37640664\n",
            "Iteration 164, loss = 0.37578412\n",
            "Iteration 165, loss = 0.37523472\n",
            "Iteration 166, loss = 0.37456951\n",
            "Iteration 167, loss = 0.37390703\n",
            "Iteration 168, loss = 0.37338797\n",
            "Iteration 169, loss = 0.37277329\n",
            "Iteration 170, loss = 0.37213228\n",
            "Iteration 171, loss = 0.37149911\n",
            "Iteration 172, loss = 0.37091439\n",
            "Iteration 173, loss = 0.37033740\n",
            "Iteration 174, loss = 0.36974789\n",
            "Iteration 175, loss = 0.36918152\n",
            "Iteration 176, loss = 0.36859813\n",
            "Iteration 177, loss = 0.36800706\n",
            "Iteration 178, loss = 0.36752112\n",
            "Iteration 179, loss = 0.36685576\n",
            "Iteration 180, loss = 0.36631029\n",
            "Iteration 181, loss = 0.36576993\n",
            "Iteration 182, loss = 0.36524049\n",
            "Iteration 183, loss = 0.36470497\n",
            "Iteration 184, loss = 0.36419290\n",
            "Iteration 185, loss = 0.36370166\n",
            "Iteration 186, loss = 0.36311607\n",
            "Iteration 187, loss = 0.36260860\n",
            "Iteration 188, loss = 0.36200300\n",
            "Iteration 189, loss = 0.36150582\n",
            "Iteration 190, loss = 0.36098907\n",
            "Iteration 191, loss = 0.36055174\n",
            "Iteration 192, loss = 0.35997130\n",
            "Iteration 193, loss = 0.35946116\n",
            "Iteration 194, loss = 0.35898583\n",
            "Iteration 195, loss = 0.35871840\n",
            "Iteration 196, loss = 0.35804022\n",
            "Iteration 197, loss = 0.35748244\n",
            "Iteration 198, loss = 0.35704247\n",
            "Iteration 199, loss = 0.35651207\n",
            "Iteration 200, loss = 0.35609144\n",
            "Iteration 201, loss = 0.35560254\n",
            "Iteration 202, loss = 0.35511041\n",
            "Iteration 203, loss = 0.35457505\n",
            "Iteration 204, loss = 0.35418313\n",
            "Iteration 205, loss = 0.35364378\n",
            "Iteration 206, loss = 0.35320613\n",
            "Iteration 207, loss = 0.35276371\n",
            "Iteration 208, loss = 0.35234012\n",
            "Iteration 209, loss = 0.35182225\n",
            "Iteration 210, loss = 0.35142870\n",
            "Iteration 211, loss = 0.35093106\n",
            "Iteration 212, loss = 0.35048841\n",
            "Iteration 213, loss = 0.35013029\n",
            "Iteration 214, loss = 0.34961309\n",
            "Iteration 215, loss = 0.34929444\n",
            "Iteration 216, loss = 0.34877694\n",
            "Iteration 217, loss = 0.34829491\n",
            "Iteration 218, loss = 0.34786809\n",
            "Iteration 219, loss = 0.34745113\n",
            "Iteration 220, loss = 0.34697974\n",
            "Iteration 221, loss = 0.34656655\n",
            "Iteration 222, loss = 0.34610635\n",
            "Iteration 223, loss = 0.34573681\n",
            "Iteration 224, loss = 0.34524103\n",
            "Iteration 225, loss = 0.34486687\n",
            "Iteration 226, loss = 0.34438505\n",
            "Iteration 227, loss = 0.34394758\n",
            "Iteration 228, loss = 0.34358117\n",
            "Iteration 229, loss = 0.34320896\n",
            "Iteration 230, loss = 0.34270287\n",
            "Iteration 231, loss = 0.34226739\n",
            "Iteration 232, loss = 0.34185660\n",
            "Iteration 233, loss = 0.34142255\n",
            "Iteration 234, loss = 0.34105178\n",
            "Iteration 235, loss = 0.34054902\n",
            "Iteration 236, loss = 0.34015293\n",
            "Iteration 237, loss = 0.33973502\n",
            "Iteration 238, loss = 0.33932469\n",
            "Iteration 239, loss = 0.33891411\n",
            "Iteration 240, loss = 0.33854553\n",
            "Iteration 241, loss = 0.33816287\n",
            "Iteration 242, loss = 0.33767607\n",
            "Iteration 243, loss = 0.33737760\n",
            "Iteration 244, loss = 0.33691331\n",
            "Iteration 245, loss = 0.33650464\n",
            "Iteration 246, loss = 0.33608052\n",
            "Iteration 247, loss = 0.33575804\n",
            "Iteration 248, loss = 0.33531589\n",
            "Iteration 249, loss = 0.33494103\n",
            "Iteration 250, loss = 0.33457823\n",
            "Iteration 251, loss = 0.33409627\n",
            "Iteration 252, loss = 0.33384482\n",
            "Iteration 253, loss = 0.33331115\n",
            "Iteration 254, loss = 0.33295100\n",
            "Iteration 255, loss = 0.33247466\n",
            "Iteration 256, loss = 0.33209997\n",
            "Iteration 257, loss = 0.33169709\n",
            "Iteration 258, loss = 0.33130600\n",
            "Iteration 259, loss = 0.33090019\n",
            "Iteration 260, loss = 0.33046715\n",
            "Iteration 261, loss = 0.33000217\n",
            "Iteration 262, loss = 0.32957360\n",
            "Iteration 263, loss = 0.32910021\n",
            "Iteration 264, loss = 0.32865555\n",
            "Iteration 265, loss = 0.32813896\n",
            "Iteration 266, loss = 0.32758543\n",
            "Iteration 267, loss = 0.32693093\n",
            "Iteration 268, loss = 0.32617165\n",
            "Iteration 269, loss = 0.32526828\n",
            "Iteration 270, loss = 0.32446445\n",
            "Iteration 271, loss = 0.32334837\n",
            "Iteration 272, loss = 0.32208765\n",
            "Iteration 273, loss = 0.32056047\n",
            "Iteration 274, loss = 0.31889537\n",
            "Iteration 275, loss = 0.31642289\n",
            "Iteration 276, loss = 0.31431567\n",
            "Iteration 277, loss = 0.31200336\n",
            "Iteration 278, loss = 0.31017724\n",
            "Iteration 279, loss = 0.30836642\n",
            "Iteration 280, loss = 0.30693016\n",
            "Iteration 281, loss = 0.30528000\n",
            "Iteration 282, loss = 0.30364651\n",
            "Iteration 283, loss = 0.30231123\n",
            "Iteration 284, loss = 0.30076784\n",
            "Iteration 285, loss = 0.29925850\n",
            "Iteration 286, loss = 0.29797683\n",
            "Iteration 287, loss = 0.29656606\n",
            "Iteration 288, loss = 0.29511364\n",
            "Iteration 289, loss = 0.29377024\n",
            "Iteration 290, loss = 0.29264682\n",
            "Iteration 291, loss = 0.29123044\n",
            "Iteration 292, loss = 0.28986747\n",
            "Iteration 293, loss = 0.28843091\n",
            "Iteration 294, loss = 0.28722553\n",
            "Iteration 295, loss = 0.28583879\n",
            "Iteration 296, loss = 0.28464981\n",
            "Iteration 297, loss = 0.28341727\n",
            "Iteration 298, loss = 0.28205731\n",
            "Iteration 299, loss = 0.28078034\n",
            "Iteration 300, loss = 0.27963768\n",
            "Iteration 301, loss = 0.27840581\n",
            "Iteration 302, loss = 0.27719320\n",
            "Iteration 303, loss = 0.27602199\n",
            "Iteration 304, loss = 0.27484588\n",
            "Iteration 305, loss = 0.27379016\n",
            "Iteration 306, loss = 0.27257630\n",
            "Iteration 307, loss = 0.27151015\n",
            "Iteration 308, loss = 0.27056010\n",
            "Iteration 309, loss = 0.26941230\n",
            "Iteration 310, loss = 0.26826322\n",
            "Iteration 311, loss = 0.26733688\n",
            "Iteration 312, loss = 0.26632595\n",
            "Iteration 313, loss = 0.26526568\n",
            "Iteration 314, loss = 0.26422536\n",
            "Iteration 315, loss = 0.26327015\n",
            "Iteration 316, loss = 0.26225353\n",
            "Iteration 317, loss = 0.26128073\n",
            "Iteration 318, loss = 0.26034216\n",
            "Iteration 319, loss = 0.25939693\n",
            "Iteration 320, loss = 0.25846264\n",
            "Iteration 321, loss = 0.25765978\n",
            "Iteration 322, loss = 0.25669418\n",
            "Iteration 323, loss = 0.25577714\n",
            "Iteration 324, loss = 0.25493475\n",
            "Iteration 325, loss = 0.25411299\n",
            "Iteration 326, loss = 0.25322845\n",
            "Iteration 327, loss = 0.25246157\n",
            "Iteration 328, loss = 0.25174839\n",
            "Iteration 329, loss = 0.25086933\n",
            "Iteration 330, loss = 0.25002519\n",
            "Iteration 331, loss = 0.24938874\n",
            "Iteration 332, loss = 0.24860343\n",
            "Iteration 333, loss = 0.24772878\n",
            "Iteration 334, loss = 0.24683836\n",
            "Iteration 335, loss = 0.24616722\n",
            "Iteration 336, loss = 0.24533942\n",
            "Iteration 337, loss = 0.24480145\n",
            "Iteration 338, loss = 0.24396923\n",
            "Iteration 339, loss = 0.24316126\n",
            "Iteration 340, loss = 0.24257505\n",
            "Iteration 341, loss = 0.24194262\n",
            "Iteration 342, loss = 0.24120888\n",
            "Iteration 343, loss = 0.24056469\n",
            "Iteration 344, loss = 0.23975486\n",
            "Iteration 345, loss = 0.23930384\n",
            "Iteration 346, loss = 0.23849342\n",
            "Iteration 347, loss = 0.23794368\n",
            "Iteration 348, loss = 0.23722395\n",
            "Iteration 349, loss = 0.23658344\n",
            "Iteration 350, loss = 0.23612215\n",
            "Iteration 351, loss = 0.23554860\n",
            "Iteration 352, loss = 0.23477825\n",
            "Iteration 353, loss = 0.23419708\n",
            "Iteration 354, loss = 0.23354087\n",
            "Iteration 355, loss = 0.23291968\n",
            "Iteration 356, loss = 0.23237056\n",
            "Iteration 357, loss = 0.23179596\n",
            "Iteration 358, loss = 0.23123707\n",
            "Iteration 359, loss = 0.23071241\n",
            "Iteration 360, loss = 0.23002404\n",
            "Iteration 361, loss = 0.22957669\n",
            "Iteration 362, loss = 0.22910317\n",
            "Iteration 363, loss = 0.22844664\n",
            "Iteration 364, loss = 0.22789397\n",
            "Iteration 365, loss = 0.22744881\n",
            "Iteration 366, loss = 0.22682266\n",
            "Iteration 367, loss = 0.22638230\n",
            "Iteration 368, loss = 0.22579712\n",
            "Iteration 369, loss = 0.22529293\n",
            "Iteration 370, loss = 0.22491788\n",
            "Iteration 371, loss = 0.22427176\n",
            "Iteration 372, loss = 0.22378821\n",
            "Iteration 373, loss = 0.22334590\n",
            "Iteration 374, loss = 0.22278544\n",
            "Iteration 375, loss = 0.22242626\n",
            "Iteration 376, loss = 0.22181616\n",
            "Iteration 377, loss = 0.22157425\n",
            "Iteration 378, loss = 0.22101516\n",
            "Iteration 379, loss = 0.22049274\n",
            "Iteration 380, loss = 0.22006397\n",
            "Iteration 381, loss = 0.21953542\n",
            "Iteration 382, loss = 0.21908665\n",
            "Iteration 383, loss = 0.21858152\n",
            "Iteration 384, loss = 0.21810945\n",
            "Iteration 385, loss = 0.21771518\n",
            "Iteration 386, loss = 0.21733333\n",
            "Iteration 387, loss = 0.21689012\n",
            "Iteration 388, loss = 0.21643591\n",
            "Iteration 389, loss = 0.21592060\n",
            "Iteration 390, loss = 0.21552654\n",
            "Iteration 391, loss = 0.21521205\n",
            "Iteration 392, loss = 0.21481676\n",
            "Iteration 393, loss = 0.21429662\n",
            "Iteration 394, loss = 0.21393002\n",
            "Iteration 395, loss = 0.21360096\n",
            "Iteration 396, loss = 0.21326401\n",
            "Iteration 397, loss = 0.21284211\n",
            "Iteration 398, loss = 0.21228298\n",
            "Iteration 399, loss = 0.21182480\n",
            "Iteration 400, loss = 0.21147530\n",
            "Iteration 401, loss = 0.21116447\n",
            "Iteration 402, loss = 0.21079929\n",
            "Iteration 403, loss = 0.21030506\n",
            "Iteration 404, loss = 0.20997615\n",
            "Iteration 405, loss = 0.20948291\n",
            "Iteration 406, loss = 0.20923660\n",
            "Iteration 407, loss = 0.20874007\n",
            "Iteration 408, loss = 0.20841837\n",
            "Iteration 409, loss = 0.20801680\n",
            "Iteration 410, loss = 0.20765394\n",
            "Iteration 411, loss = 0.20731315\n",
            "Iteration 412, loss = 0.20704367\n",
            "Iteration 413, loss = 0.20653435\n",
            "Iteration 414, loss = 0.20619827\n",
            "Iteration 415, loss = 0.20587553\n",
            "Iteration 416, loss = 0.20550409\n",
            "Iteration 417, loss = 0.20508400\n",
            "Iteration 418, loss = 0.20474952\n",
            "Iteration 419, loss = 0.20448199\n",
            "Iteration 420, loss = 0.20417479\n",
            "Iteration 421, loss = 0.20363971\n",
            "Iteration 422, loss = 0.20331447\n",
            "Iteration 423, loss = 0.20296193\n",
            "Iteration 424, loss = 0.20276925\n",
            "Iteration 425, loss = 0.20236262\n",
            "Iteration 426, loss = 0.20206181\n",
            "Iteration 427, loss = 0.20163088\n",
            "Iteration 428, loss = 0.20130348\n",
            "Iteration 429, loss = 0.20103677\n",
            "Iteration 430, loss = 0.20070578\n",
            "Iteration 431, loss = 0.20054120\n",
            "Iteration 432, loss = 0.20009009\n",
            "Iteration 433, loss = 0.19988882\n",
            "Iteration 434, loss = 0.19949333\n",
            "Iteration 435, loss = 0.19909614\n",
            "Iteration 436, loss = 0.19882298\n",
            "Iteration 437, loss = 0.19843623\n",
            "Iteration 438, loss = 0.19815896\n",
            "Iteration 439, loss = 0.19792026\n",
            "Iteration 440, loss = 0.19754651\n",
            "Iteration 441, loss = 0.19724676\n",
            "Iteration 442, loss = 0.19705946\n",
            "Iteration 443, loss = 0.19658489\n",
            "Iteration 444, loss = 0.19631787\n",
            "Iteration 445, loss = 0.19614203\n",
            "Iteration 446, loss = 0.19585896\n",
            "Iteration 447, loss = 0.19546469\n",
            "Iteration 448, loss = 0.19518705\n",
            "Iteration 449, loss = 0.19493676\n",
            "Iteration 450, loss = 0.19460493\n",
            "Iteration 451, loss = 0.19462836\n",
            "Iteration 452, loss = 0.19400303\n",
            "Iteration 453, loss = 0.19378096\n",
            "Iteration 454, loss = 0.19354861\n",
            "Iteration 455, loss = 0.19323910\n",
            "Iteration 456, loss = 0.19277251\n",
            "Iteration 457, loss = 0.19259802\n",
            "Iteration 458, loss = 0.19234548\n",
            "Iteration 459, loss = 0.19218445\n",
            "Iteration 460, loss = 0.19169476\n",
            "Iteration 461, loss = 0.19138457\n",
            "Iteration 462, loss = 0.19119359\n",
            "Iteration 463, loss = 0.19090587\n",
            "Iteration 464, loss = 0.19062596\n",
            "Iteration 465, loss = 0.19034644\n",
            "Iteration 466, loss = 0.19001806\n",
            "Iteration 467, loss = 0.18978750\n",
            "Iteration 468, loss = 0.18948306\n",
            "Iteration 469, loss = 0.18930855\n",
            "Iteration 470, loss = 0.18898431\n",
            "Iteration 471, loss = 0.18873802\n",
            "Iteration 472, loss = 0.18850204\n",
            "Iteration 473, loss = 0.18826359\n",
            "Iteration 474, loss = 0.18791948\n",
            "Iteration 475, loss = 0.18769600\n",
            "Iteration 476, loss = 0.18755461\n",
            "Iteration 477, loss = 0.18724898\n",
            "Iteration 478, loss = 0.18706346\n",
            "Iteration 479, loss = 0.18674624\n",
            "Iteration 480, loss = 0.18644641\n",
            "Iteration 481, loss = 0.18617928\n",
            "Iteration 482, loss = 0.18633386\n",
            "Iteration 483, loss = 0.18568549\n",
            "Iteration 484, loss = 0.18548495\n",
            "Iteration 485, loss = 0.18528883\n",
            "Iteration 486, loss = 0.18502099\n",
            "Iteration 487, loss = 0.18480148\n",
            "Iteration 488, loss = 0.18456794\n",
            "Iteration 489, loss = 0.18426693\n",
            "Iteration 490, loss = 0.18411754\n",
            "Iteration 491, loss = 0.18384369\n",
            "Iteration 492, loss = 0.18366798\n",
            "Iteration 493, loss = 0.18359839\n",
            "Iteration 494, loss = 0.18310351\n",
            "Iteration 495, loss = 0.18288651\n",
            "Iteration 496, loss = 0.18284629\n",
            "Iteration 497, loss = 0.18259482\n",
            "Iteration 498, loss = 0.18226993\n",
            "Iteration 499, loss = 0.18199975\n",
            "Iteration 500, loss = 0.18179786\n",
            "Iteration 1, loss = 1.00850489\n",
            "Iteration 2, loss = 0.96567804\n",
            "Iteration 3, loss = 0.92482782\n",
            "Iteration 4, loss = 0.88553719\n",
            "Iteration 5, loss = 0.84792000\n",
            "Iteration 6, loss = 0.81144732\n",
            "Iteration 7, loss = 0.77564514\n",
            "Iteration 8, loss = 0.74138810\n",
            "Iteration 9, loss = 0.70744521\n",
            "Iteration 10, loss = 0.67325506\n",
            "Iteration 11, loss = 0.63939740\n",
            "Iteration 12, loss = 0.60477019\n",
            "Iteration 13, loss = 0.57027419\n",
            "Iteration 14, loss = 0.53588634\n",
            "Iteration 15, loss = 0.50288141\n",
            "Iteration 16, loss = 0.47003579\n",
            "Iteration 17, loss = 0.43802736\n",
            "Iteration 18, loss = 0.40818530\n",
            "Iteration 19, loss = 0.37975675\n",
            "Iteration 20, loss = 0.35378338\n",
            "Iteration 21, loss = 0.33037636\n",
            "Iteration 22, loss = 0.30844953\n",
            "Iteration 23, loss = 0.28880009\n",
            "Iteration 24, loss = 0.27123904\n",
            "Iteration 25, loss = 0.25548320\n",
            "Iteration 26, loss = 0.24151084\n",
            "Iteration 27, loss = 0.22892568\n",
            "Iteration 28, loss = 0.21842529\n",
            "Iteration 29, loss = 0.20903998\n",
            "Iteration 30, loss = 0.20081794\n",
            "Iteration 31, loss = 0.19368252\n",
            "Iteration 32, loss = 0.18744354\n",
            "Iteration 33, loss = 0.18199369\n",
            "Iteration 34, loss = 0.17706880\n",
            "Iteration 35, loss = 0.17276504\n",
            "Iteration 36, loss = 0.16903878\n",
            "Iteration 37, loss = 0.16592876\n",
            "Iteration 38, loss = 0.16315631\n",
            "Iteration 39, loss = 0.16073472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 40, loss = 0.15852062\n",
            "Iteration 41, loss = 0.15656077\n",
            "Iteration 42, loss = 0.15475890\n",
            "Iteration 43, loss = 0.15321092\n",
            "Iteration 44, loss = 0.15172805\n",
            "Iteration 45, loss = 0.15042131\n",
            "Iteration 46, loss = 0.14921140\n",
            "Iteration 47, loss = 0.14797067\n",
            "Iteration 48, loss = 0.14688543\n",
            "Iteration 49, loss = 0.14577593\n",
            "Iteration 50, loss = 0.14476853\n",
            "Iteration 51, loss = 0.14374647\n",
            "Iteration 52, loss = 0.14279802\n",
            "Iteration 53, loss = 0.14181863\n",
            "Iteration 54, loss = 0.14085335\n",
            "Iteration 55, loss = 0.13993392\n",
            "Iteration 56, loss = 0.13902460\n",
            "Iteration 57, loss = 0.13809719\n",
            "Iteration 58, loss = 0.13720232\n",
            "Iteration 59, loss = 0.13629544\n",
            "Iteration 60, loss = 0.13542198\n",
            "Iteration 61, loss = 0.13459827\n",
            "Iteration 62, loss = 0.13374843\n",
            "Iteration 63, loss = 0.13286737\n",
            "Iteration 64, loss = 0.13206986\n",
            "Iteration 65, loss = 0.13131480\n",
            "Iteration 66, loss = 0.13051730\n",
            "Iteration 67, loss = 0.12974826\n",
            "Iteration 68, loss = 0.12899123\n",
            "Iteration 69, loss = 0.12824332\n",
            "Iteration 70, loss = 0.12751512\n",
            "Iteration 71, loss = 0.12684712\n",
            "Iteration 72, loss = 0.12612853\n",
            "Iteration 73, loss = 0.12547393\n",
            "Iteration 74, loss = 0.12480527\n",
            "Iteration 75, loss = 0.12413407\n",
            "Iteration 76, loss = 0.12351050\n",
            "Iteration 77, loss = 0.12288677\n",
            "Iteration 78, loss = 0.12226475\n",
            "Iteration 79, loss = 0.12166562\n",
            "Iteration 80, loss = 0.12106465\n",
            "Iteration 81, loss = 0.12049885\n",
            "Iteration 82, loss = 0.11988088\n",
            "Iteration 83, loss = 0.11934407\n",
            "Iteration 84, loss = 0.11878116\n",
            "Iteration 85, loss = 0.11822841\n",
            "Iteration 86, loss = 0.11766447\n",
            "Iteration 87, loss = 0.11717978\n",
            "Iteration 88, loss = 0.11663563\n",
            "Iteration 89, loss = 0.11614927\n",
            "Iteration 90, loss = 0.11562058\n",
            "Iteration 91, loss = 0.11517122\n",
            "Iteration 92, loss = 0.11465985\n",
            "Iteration 93, loss = 0.11420290\n",
            "Iteration 94, loss = 0.11365607\n",
            "Iteration 95, loss = 0.11322675\n",
            "Iteration 96, loss = 0.11275200\n",
            "Iteration 97, loss = 0.11228442\n",
            "Iteration 98, loss = 0.11183750\n",
            "Iteration 99, loss = 0.11137255\n",
            "Iteration 100, loss = 0.11092217\n",
            "Iteration 101, loss = 0.11049225\n",
            "Iteration 102, loss = 0.11005996\n",
            "Iteration 103, loss = 0.10965366\n",
            "Iteration 104, loss = 0.10921242\n",
            "Iteration 105, loss = 0.10880492\n",
            "Iteration 106, loss = 0.10843877\n",
            "Iteration 107, loss = 0.10797006\n",
            "Iteration 108, loss = 0.10757109\n",
            "Iteration 109, loss = 0.10716257\n",
            "Iteration 110, loss = 0.10680466\n",
            "Iteration 111, loss = 0.10640239\n",
            "Iteration 112, loss = 0.10602765\n",
            "Iteration 113, loss = 0.10561631\n",
            "Iteration 114, loss = 0.10528546\n",
            "Iteration 115, loss = 0.10491366\n",
            "Iteration 116, loss = 0.10454631\n",
            "Iteration 117, loss = 0.10422547\n",
            "Iteration 118, loss = 0.10384816\n",
            "Iteration 119, loss = 0.10353823\n",
            "Iteration 120, loss = 0.10316291\n",
            "Iteration 121, loss = 0.10287351\n",
            "Iteration 122, loss = 0.10252466\n",
            "Iteration 123, loss = 0.10219780\n",
            "Iteration 124, loss = 0.10189259\n",
            "Iteration 125, loss = 0.10158940\n",
            "Iteration 126, loss = 0.10128123\n",
            "Iteration 127, loss = 0.10098831\n",
            "Iteration 128, loss = 0.10070451\n",
            "Iteration 129, loss = 0.10039512\n",
            "Iteration 130, loss = 0.10010193\n",
            "Iteration 131, loss = 0.09980052\n",
            "Iteration 132, loss = 0.09949948\n",
            "Iteration 133, loss = 0.09922044\n",
            "Iteration 134, loss = 0.09893815\n",
            "Iteration 135, loss = 0.09868356\n",
            "Iteration 136, loss = 0.09839186\n",
            "Iteration 137, loss = 0.09814185\n",
            "Iteration 138, loss = 0.09788012\n",
            "Iteration 139, loss = 0.09759757\n",
            "Iteration 140, loss = 0.09732512\n",
            "Iteration 141, loss = 0.09707416\n",
            "Iteration 142, loss = 0.09681465\n",
            "Iteration 143, loss = 0.09658308\n",
            "Iteration 144, loss = 0.09632791\n",
            "Iteration 145, loss = 0.09611825\n",
            "Iteration 146, loss = 0.09584776\n",
            "Iteration 147, loss = 0.09562090\n",
            "Iteration 148, loss = 0.09537967\n",
            "Iteration 149, loss = 0.09514845\n",
            "Iteration 150, loss = 0.09490226\n",
            "Iteration 151, loss = 0.09469134\n",
            "Iteration 152, loss = 0.09449007\n",
            "Iteration 153, loss = 0.09428102\n",
            "Iteration 154, loss = 0.09407025\n",
            "Iteration 155, loss = 0.09384752\n",
            "Iteration 156, loss = 0.09365139\n",
            "Iteration 157, loss = 0.09342994\n",
            "Iteration 158, loss = 0.09325348\n",
            "Iteration 159, loss = 0.09301845\n",
            "Iteration 160, loss = 0.09282023\n",
            "Iteration 161, loss = 0.09263018\n",
            "Iteration 162, loss = 0.09243935\n",
            "Iteration 163, loss = 0.09228305\n",
            "Iteration 164, loss = 0.09208729\n",
            "Iteration 165, loss = 0.09190807\n",
            "Iteration 166, loss = 0.09175561\n",
            "Iteration 167, loss = 0.09155834\n",
            "Iteration 168, loss = 0.09140779\n",
            "Iteration 169, loss = 0.09123842\n",
            "Iteration 170, loss = 0.09106551\n",
            "Iteration 171, loss = 0.09089686\n",
            "Iteration 172, loss = 0.09073186\n",
            "Iteration 173, loss = 0.09062346\n",
            "Iteration 174, loss = 0.09042761\n",
            "Iteration 175, loss = 0.09023435\n",
            "Iteration 176, loss = 0.09010091\n",
            "Iteration 177, loss = 0.08990385\n",
            "Iteration 178, loss = 0.08975486\n",
            "Iteration 179, loss = 0.08959503\n",
            "Iteration 180, loss = 0.08946727\n",
            "Iteration 181, loss = 0.08928887\n",
            "Iteration 182, loss = 0.08913321\n",
            "Iteration 183, loss = 0.08898717\n",
            "Iteration 184, loss = 0.08886768\n",
            "Iteration 185, loss = 0.08871142\n",
            "Iteration 186, loss = 0.08858948\n",
            "Iteration 187, loss = 0.08843797\n",
            "Iteration 188, loss = 0.08829397\n",
            "Iteration 189, loss = 0.08815309\n",
            "Iteration 190, loss = 0.08804272\n",
            "Iteration 191, loss = 0.08788556\n",
            "Iteration 192, loss = 0.08774350\n",
            "Iteration 193, loss = 0.08761791\n",
            "Iteration 194, loss = 0.08750291\n",
            "Iteration 195, loss = 0.08735707\n",
            "Iteration 196, loss = 0.08723589\n",
            "Iteration 197, loss = 0.08712298\n",
            "Iteration 198, loss = 0.08699042\n",
            "Iteration 199, loss = 0.08687116\n",
            "Iteration 200, loss = 0.08675035\n",
            "Iteration 201, loss = 0.08663931\n",
            "Iteration 202, loss = 0.08650354\n",
            "Iteration 203, loss = 0.08638826\n",
            "Iteration 204, loss = 0.08628798\n",
            "Iteration 205, loss = 0.08615503\n",
            "Iteration 206, loss = 0.08602652\n",
            "Iteration 207, loss = 0.08591496\n",
            "Iteration 208, loss = 0.08582049\n",
            "Iteration 209, loss = 0.08567654\n",
            "Iteration 210, loss = 0.08556706\n",
            "Iteration 211, loss = 0.08543768\n",
            "Iteration 212, loss = 0.08532053\n",
            "Iteration 213, loss = 0.08521799\n",
            "Iteration 214, loss = 0.08510020\n",
            "Iteration 215, loss = 0.08498558\n",
            "Iteration 216, loss = 0.08485871\n",
            "Iteration 217, loss = 0.08475110\n",
            "Iteration 218, loss = 0.08466261\n",
            "Iteration 219, loss = 0.08458124\n",
            "Iteration 220, loss = 0.08441716\n",
            "Iteration 221, loss = 0.08430377\n",
            "Iteration 222, loss = 0.08423889\n",
            "Iteration 223, loss = 0.08410329\n",
            "Iteration 224, loss = 0.08401080\n",
            "Iteration 225, loss = 0.08388536\n",
            "Iteration 226, loss = 0.08377168\n",
            "Iteration 227, loss = 0.08366307\n",
            "Iteration 228, loss = 0.08357831\n",
            "Iteration 229, loss = 0.08349360\n",
            "Iteration 230, loss = 0.08336016\n",
            "Iteration 231, loss = 0.08326069\n",
            "Iteration 232, loss = 0.08316456\n",
            "Iteration 233, loss = 0.08306382\n",
            "Iteration 234, loss = 0.08298379\n",
            "Iteration 235, loss = 0.08287345\n",
            "Iteration 236, loss = 0.08279179\n",
            "Iteration 237, loss = 0.08268635\n",
            "Iteration 238, loss = 0.08259126\n",
            "Iteration 239, loss = 0.08249413\n",
            "Iteration 240, loss = 0.08241520\n",
            "Iteration 241, loss = 0.08230813\n",
            "Iteration 242, loss = 0.08223886\n",
            "Iteration 243, loss = 0.08213587\n",
            "Iteration 244, loss = 0.08208046\n",
            "Iteration 245, loss = 0.08195878\n",
            "Iteration 246, loss = 0.08189329\n",
            "Iteration 247, loss = 0.08179191\n",
            "Iteration 248, loss = 0.08169737\n",
            "Iteration 249, loss = 0.08160864\n",
            "Iteration 250, loss = 0.08151891\n",
            "Iteration 251, loss = 0.08143819\n",
            "Iteration 252, loss = 0.08136006\n",
            "Iteration 253, loss = 0.08126823\n",
            "Iteration 254, loss = 0.08117679\n",
            "Iteration 255, loss = 0.08112534\n",
            "Iteration 256, loss = 0.08098788\n",
            "Iteration 257, loss = 0.08092749\n",
            "Iteration 258, loss = 0.08081415\n",
            "Iteration 259, loss = 0.08073313\n",
            "Iteration 260, loss = 0.08064175\n",
            "Iteration 261, loss = 0.08056913\n",
            "Iteration 262, loss = 0.08046206\n",
            "Iteration 263, loss = 0.08038446\n",
            "Iteration 264, loss = 0.08033706\n",
            "Iteration 265, loss = 0.08025766\n",
            "Iteration 266, loss = 0.08015284\n",
            "Iteration 267, loss = 0.08003277\n",
            "Iteration 268, loss = 0.07995621\n",
            "Iteration 269, loss = 0.07987678\n",
            "Iteration 270, loss = 0.07981491\n",
            "Iteration 271, loss = 0.07971451\n",
            "Iteration 272, loss = 0.07969904\n",
            "Iteration 273, loss = 0.07953945\n",
            "Iteration 274, loss = 0.07946642\n",
            "Iteration 275, loss = 0.07939654\n",
            "Iteration 276, loss = 0.07931574\n",
            "Iteration 277, loss = 0.07921934\n",
            "Iteration 278, loss = 0.07914835\n",
            "Iteration 279, loss = 0.07908188\n",
            "Iteration 280, loss = 0.07898398\n",
            "Iteration 281, loss = 0.07890646\n",
            "Iteration 282, loss = 0.07885610\n",
            "Iteration 283, loss = 0.07876202\n",
            "Iteration 284, loss = 0.07869047\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training Final Percent Accuracy: 84.43%\n",
            "Testing Final Percent Accuracy: 84.39%\n",
            "Final Log Loss on Test Set: 0.3994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwR9gmCr-3z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}